{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebvqJaNU9bkH"
      },
      "source": [
        "# Wprowadzenie do sieci neuronowych i uczenia maszynowego - Sieci Rekurencyjne\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Prowadzący:** Piotr Baryczkowski, Jakub Bednarek<br>\n",
        "**Kontakt:** piotr.baryczkowski@put.poznan.pl<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlq47LA0BuBB"
      },
      "source": [
        "## Cel ćwiczeń:\n",
        "- zapoznanie się z rekurencyjnymi sieciami neuronowymi,\n",
        "- stworzenie modelu sieci z warstwami rekurencyjnymi dla zbioru danych MNIST,\n",
        "- stworzenie własnych implementacji warstwami neuronowymi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxLU8paIDmUe",
        "outputId": "4b45a229-6a1f-4e1b-ffb2-d7f9f6dcc031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mzTzdZHPfEkv"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wV_u-YBWEJ8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2eb71c2-4265-40a6-a910-fe9af278816f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 348kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.21MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.37MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppmDSGoyFuJ9"
      },
      "source": [
        "## Sieci rekurencyjne\n",
        "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "\n",
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
        "\n",
        "Przykładowy model z warstwą rekurencyjną dla danych MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_YKDA5VkfEkw",
        "outputId": "496f6e1a-b07c-4756-c360-2f85396ccd65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel(\n",
              "  (lstm_1): LSTM(28, 128, batch_first=True)\n",
              "  (relu_1): ReLU()\n",
              "  (dense_1): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "class RecurrentModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModel, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # Define your layers here.\n",
        "        self.lstm_1 = nn.LSTM(input_size=28, hidden_size=128, batch_first=True)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        lstm_out, _ = self.lstm_1(inputs)\n",
        "        # Take the last output from the sequence (assume inputs are padded appropriately or have consistent lengths)\n",
        "        x = lstm_out[:, -1, :]  # Get the output of the last time step\n",
        "        x = self.relu_1(x)\n",
        "        x = self.dense_1(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "model = RecurrentModel(num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EgXZxT7SfEkw"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t-L7ZTjWfEkx",
        "outputId": "fd752f8c-b583-4ae5-a227-4258e4621e43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304932  [   32/60000]\n",
            "loss: 1.896042  [ 3232/60000]\n",
            "loss: 2.089072  [ 6432/60000]\n",
            "loss: 1.877084  [ 9632/60000]\n",
            "loss: 1.762566  [12832/60000]\n",
            "loss: 1.632664  [16032/60000]\n",
            "loss: 1.704073  [19232/60000]\n",
            "loss: 1.753190  [22432/60000]\n",
            "loss: 1.852072  [25632/60000]\n",
            "loss: 1.625864  [28832/60000]\n",
            "loss: 1.623577  [32032/60000]\n",
            "loss: 1.636896  [35232/60000]\n",
            "loss: 1.689724  [38432/60000]\n",
            "loss: 1.597557  [41632/60000]\n",
            "loss: 1.639373  [44832/60000]\n",
            "loss: 1.581354  [48032/60000]\n",
            "loss: 1.562089  [51232/60000]\n",
            "loss: 1.594318  [54432/60000]\n",
            "loss: 1.553415  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.1%, Avg loss: 1.532177 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.519638  [   32/60000]\n",
            "loss: 1.627392  [ 3232/60000]\n",
            "loss: 1.531672  [ 6432/60000]\n",
            "loss: 1.642433  [ 9632/60000]\n",
            "loss: 1.581258  [12832/60000]\n",
            "loss: 1.466676  [16032/60000]\n",
            "loss: 1.540027  [19232/60000]\n",
            "loss: 1.546480  [22432/60000]\n",
            "loss: 1.491116  [25632/60000]\n",
            "loss: 1.526742  [28832/60000]\n",
            "loss: 1.493049  [32032/60000]\n",
            "loss: 1.558165  [35232/60000]\n",
            "loss: 1.551441  [38432/60000]\n",
            "loss: 1.523710  [41632/60000]\n",
            "loss: 1.492095  [44832/60000]\n",
            "loss: 1.492544  [48032/60000]\n",
            "loss: 1.511377  [51232/60000]\n",
            "loss: 1.492648  [54432/60000]\n",
            "loss: 1.466840  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 1.507963 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.588236  [   32/60000]\n",
            "loss: 1.514194  [ 3232/60000]\n",
            "loss: 1.577868  [ 6432/60000]\n",
            "loss: 1.583535  [ 9632/60000]\n",
            "loss: 1.493908  [12832/60000]\n",
            "loss: 1.494858  [16032/60000]\n",
            "loss: 1.493394  [19232/60000]\n",
            "loss: 1.525070  [22432/60000]\n",
            "loss: 1.556027  [25632/60000]\n",
            "loss: 1.557322  [28832/60000]\n",
            "loss: 1.492963  [32032/60000]\n",
            "loss: 1.497252  [35232/60000]\n",
            "loss: 1.513565  [38432/60000]\n",
            "loss: 1.522298  [41632/60000]\n",
            "loss: 1.497992  [44832/60000]\n",
            "loss: 1.474664  [48032/60000]\n",
            "loss: 1.524300  [51232/60000]\n",
            "loss: 1.488227  [54432/60000]\n",
            "loss: 1.556237  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.3%, Avg loss: 1.508695 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.461763  [   32/60000]\n",
            "loss: 1.509247  [ 3232/60000]\n",
            "loss: 1.492053  [ 6432/60000]\n",
            "loss: 1.494422  [ 9632/60000]\n",
            "loss: 1.493608  [12832/60000]\n",
            "loss: 1.526127  [16032/60000]\n",
            "loss: 1.555342  [19232/60000]\n",
            "loss: 1.554829  [22432/60000]\n",
            "loss: 1.498475  [25632/60000]\n",
            "loss: 1.472898  [28832/60000]\n",
            "loss: 1.511417  [32032/60000]\n",
            "loss: 1.513362  [35232/60000]\n",
            "loss: 1.518364  [38432/60000]\n",
            "loss: 1.551715  [41632/60000]\n",
            "loss: 1.522684  [44832/60000]\n",
            "loss: 1.508816  [48032/60000]\n",
            "loss: 1.492597  [51232/60000]\n",
            "loss: 1.487974  [54432/60000]\n",
            "loss: 1.584923  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.1%, Avg loss: 1.500481 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.523645  [   32/60000]\n",
            "loss: 1.497774  [ 3232/60000]\n",
            "loss: 1.525995  [ 6432/60000]\n",
            "loss: 1.512511  [ 9632/60000]\n",
            "loss: 1.555217  [12832/60000]\n",
            "loss: 1.461249  [16032/60000]\n",
            "loss: 1.533577  [19232/60000]\n",
            "loss: 1.507530  [22432/60000]\n",
            "loss: 1.461213  [25632/60000]\n",
            "loss: 1.492588  [28832/60000]\n",
            "loss: 1.523768  [32032/60000]\n",
            "loss: 1.507630  [35232/60000]\n",
            "loss: 1.547534  [38432/60000]\n",
            "loss: 1.492434  [41632/60000]\n",
            "loss: 1.461248  [44832/60000]\n",
            "loss: 1.523681  [48032/60000]\n",
            "loss: 1.523501  [51232/60000]\n",
            "loss: 1.461197  [54432/60000]\n",
            "loss: 1.548536  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 1.504526 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgtZzVYg1361"
      },
      "source": [
        "### Zadanie 1\n",
        "Rozszerz model z powyższego przykładu o kolejną warstwę rekurencyjną przed gęstą warstwą wyjściową.\n",
        "\n",
        "Standardowe sieci neuronowe generują jeden wynik na podstawie jednego inputu.\n",
        "Natomiast sieci rekurencyjne przetwarzają dane sekwencyjnie, w każdym kroku łącząc wynik poprzedniego przetwarzania i aktualnego wejścia. Dlatego domyślnym wejściem sieci neuronowej jest tensor 3-wymiarowy ([batch_size,sequence_size,sample_size]).\n",
        "Domyślnie warstwy rekurencyjne w PyTorchu zwracają sekwencje wyników wszystkich kroków przetwarzania dla warstwy rekurencyjnej. Jeśli chcesz zwrócić tylko wyniki ostatniego przetwarzania dla warstwy rekurencyjnej, musisz samemu to zaimplementować np. `x = lstm_out[:, -1, :]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FzMsg5A7fEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d486a397-ed3d-43a6-f588-2f5d926418e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel2(\n",
              "  (lstm1): LSTM(28, 128, batch_first=True)\n",
              "  (relu): ReLU()\n",
              "  (lstm2): LSTM(128, 128, batch_first=True)\n",
              "  (dense): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "class RecurrentModel2(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModel2, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.lstm1 = nn.LSTM(input_size=28, hidden_size=128, batch_first=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=128, batch_first=True)\n",
        "        self.dense = nn.Linear(in_features=128, out_features=num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        lstm1_out, _ = self.lstm1(inputs)\n",
        "        x = self.relu(lstm1_out)\n",
        "        # Second LSTM layer\n",
        "        lstm2_out, _ = self.lstm2(x)\n",
        "        x = lstm2_out[:,-1,:]\n",
        "        x = self.relu(x)\n",
        "        out = self.dense(x)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "model = RecurrentModel2(num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3ptuv6IHfEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f289a8-1aa4-40c5-b8e8-aac2033714d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.301903  [   32/60000]\n",
            "loss: 2.141454  [ 3232/60000]\n",
            "loss: 1.767656  [ 6432/60000]\n",
            "loss: 1.733031  [ 9632/60000]\n",
            "loss: 1.748597  [12832/60000]\n",
            "loss: 1.753097  [16032/60000]\n",
            "loss: 1.683070  [19232/60000]\n",
            "loss: 1.606607  [22432/60000]\n",
            "loss: 1.686070  [25632/60000]\n",
            "loss: 1.560992  [28832/60000]\n",
            "loss: 1.533781  [32032/60000]\n",
            "loss: 1.641106  [35232/60000]\n",
            "loss: 1.576625  [38432/60000]\n",
            "loss: 1.631911  [41632/60000]\n",
            "loss: 1.580217  [44832/60000]\n",
            "loss: 1.525512  [48032/60000]\n",
            "loss: 1.514617  [51232/60000]\n",
            "loss: 1.496857  [54432/60000]\n",
            "loss: 1.563026  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 90.4%, Avg loss: 1.558206 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.538977  [   32/60000]\n",
            "loss: 1.597736  [ 3232/60000]\n",
            "loss: 1.506887  [ 6432/60000]\n",
            "loss: 1.526548  [ 9632/60000]\n",
            "loss: 1.497256  [12832/60000]\n",
            "loss: 1.495361  [16032/60000]\n",
            "loss: 1.587017  [19232/60000]\n",
            "loss: 1.493157  [22432/60000]\n",
            "loss: 1.703901  [25632/60000]\n",
            "loss: 1.461652  [28832/60000]\n",
            "loss: 1.532627  [32032/60000]\n",
            "loss: 1.525628  [35232/60000]\n",
            "loss: 1.488386  [38432/60000]\n",
            "loss: 1.492900  [41632/60000]\n",
            "loss: 1.533712  [44832/60000]\n",
            "loss: 1.601646  [48032/60000]\n",
            "loss: 1.569622  [51232/60000]\n",
            "loss: 1.461272  [54432/60000]\n",
            "loss: 1.508488  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 1.519078 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.499636  [   32/60000]\n",
            "loss: 1.482118  [ 3232/60000]\n",
            "loss: 1.493800  [ 6432/60000]\n",
            "loss: 1.512595  [ 9632/60000]\n",
            "loss: 1.462348  [12832/60000]\n",
            "loss: 1.520446  [16032/60000]\n",
            "loss: 1.526587  [19232/60000]\n",
            "loss: 1.591438  [22432/60000]\n",
            "loss: 1.532926  [25632/60000]\n",
            "loss: 1.526809  [28832/60000]\n",
            "loss: 1.492446  [32032/60000]\n",
            "loss: 1.463817  [35232/60000]\n",
            "loss: 1.536335  [38432/60000]\n",
            "loss: 1.518524  [41632/60000]\n",
            "loss: 1.479328  [44832/60000]\n",
            "loss: 1.502254  [48032/60000]\n",
            "loss: 1.465266  [51232/60000]\n",
            "loss: 1.461453  [54432/60000]\n",
            "loss: 1.523348  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 1.504041 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.505135  [   32/60000]\n",
            "loss: 1.469026  [ 3232/60000]\n",
            "loss: 1.462068  [ 6432/60000]\n",
            "loss: 1.461624  [ 9632/60000]\n",
            "loss: 1.480665  [12832/60000]\n",
            "loss: 1.487458  [16032/60000]\n",
            "loss: 1.492625  [19232/60000]\n",
            "loss: 1.546943  [22432/60000]\n",
            "loss: 1.554072  [25632/60000]\n",
            "loss: 1.520701  [28832/60000]\n",
            "loss: 1.494186  [32032/60000]\n",
            "loss: 1.463946  [35232/60000]\n",
            "loss: 1.492528  [38432/60000]\n",
            "loss: 1.477330  [41632/60000]\n",
            "loss: 1.492673  [44832/60000]\n",
            "loss: 1.461223  [48032/60000]\n",
            "loss: 1.492433  [51232/60000]\n",
            "loss: 1.461555  [54432/60000]\n",
            "loss: 1.484228  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.1%, Avg loss: 1.499996 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.492348  [   32/60000]\n",
            "loss: 1.466877  [ 3232/60000]\n",
            "loss: 1.484908  [ 6432/60000]\n",
            "loss: 1.464986  [ 9632/60000]\n",
            "loss: 1.492572  [12832/60000]\n",
            "loss: 1.499850  [16032/60000]\n",
            "loss: 1.488105  [19232/60000]\n",
            "loss: 1.552484  [22432/60000]\n",
            "loss: 1.518778  [25632/60000]\n",
            "loss: 1.461549  [28832/60000]\n",
            "loss: 1.491857  [32032/60000]\n",
            "loss: 1.466479  [35232/60000]\n",
            "loss: 1.492413  [38432/60000]\n",
            "loss: 1.574785  [41632/60000]\n",
            "loss: 1.524300  [44832/60000]\n",
            "loss: 1.461199  [48032/60000]\n",
            "loss: 1.511955  [51232/60000]\n",
            "loss: 1.492456  [54432/60000]\n",
            "loss: 1.492699  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 1.487846 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYDLWjdseB4H"
      },
      "source": [
        "### Zadanie 2\n",
        "Wykorzystując model z przykładu, napisz sieć rekurencyjną przy użyciu RNNCell.\n",
        "\n",
        "RNNCell implementuje tylko operacje wykonywane przez warstwę\n",
        "rekurencyjną dla jednego kroku. Warstwy rekurencyjne w każdym kroku\n",
        "łączą wynik operacji poprzedniego kroku i aktualny input.\n",
        "Wykorzystaj pętle for do wielokrotnego wywołania komórki RNNCell (liczba kroków to liczba elementów w sekwencji).\n",
        "\n",
        "Wywołanie zainicjalizowanej komórki rekurencyjnej wymaga podania aktualnego inputu i listy stanów ukrytych poprzedniego kroku (RNNCell ma jeden stan).\n",
        "\n",
        "Trzeba zainicjalizować ukryty stan warstwy z wartościami początkowymi (można wykorzystać zmienne losowe - torch.rand)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UXwkHVXLfEky"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RecurrentModel3(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.rnnCell = nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        self.dense = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        batch_size, sequence_length, features = inputs.size()\n",
        "        hidden_state = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
        "        for t in range(sequence_length):\n",
        "            input_t = inputs[:,t,:]\n",
        "            hidden_state = self.rnnCell(input_t, hidden_state)\n",
        "        out = self.dense(hidden_state)\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "model = RecurrentModel3(input_size=28, hidden_size=128, num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CVmL0U34fEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ba69ca-f2f0-4a4d-a2a5-702c1a263ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.303452  [   32/60000]\n",
            "loss: 2.322311  [ 3232/60000]\n",
            "loss: 2.197619  [ 6432/60000]\n",
            "loss: 2.147569  [ 9632/60000]\n",
            "loss: 2.077339  [12832/60000]\n",
            "loss: 2.183924  [16032/60000]\n",
            "loss: 2.231068  [19232/60000]\n",
            "loss: 1.962229  [22432/60000]\n",
            "loss: 2.066641  [25632/60000]\n",
            "loss: 2.072934  [28832/60000]\n",
            "loss: 1.846014  [32032/60000]\n",
            "loss: 1.976134  [35232/60000]\n",
            "loss: 2.195160  [38432/60000]\n",
            "loss: 1.955630  [41632/60000]\n",
            "loss: 1.879447  [44832/60000]\n",
            "loss: 1.852863  [48032/60000]\n",
            "loss: 1.834392  [51232/60000]\n",
            "loss: 1.888616  [54432/60000]\n",
            "loss: 1.896469  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.8%, Avg loss: 1.794687 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.858550  [   32/60000]\n",
            "loss: 1.907042  [ 3232/60000]\n",
            "loss: 1.783405  [ 6432/60000]\n",
            "loss: 1.925390  [ 9632/60000]\n",
            "loss: 1.669199  [12832/60000]\n",
            "loss: 1.876804  [16032/60000]\n",
            "loss: 1.813879  [19232/60000]\n",
            "loss: 1.793387  [22432/60000]\n",
            "loss: 1.710211  [25632/60000]\n",
            "loss: 1.740881  [28832/60000]\n",
            "loss: 1.692290  [32032/60000]\n",
            "loss: 1.774662  [35232/60000]\n",
            "loss: 1.898054  [38432/60000]\n",
            "loss: 1.638064  [41632/60000]\n",
            "loss: 1.779216  [44832/60000]\n",
            "loss: 1.635156  [48032/60000]\n",
            "loss: 1.864427  [51232/60000]\n",
            "loss: 1.837336  [54432/60000]\n",
            "loss: 1.793327  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.4%, Avg loss: 1.708475 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.774968  [   32/60000]\n",
            "loss: 1.739480  [ 3232/60000]\n",
            "loss: 1.705184  [ 6432/60000]\n",
            "loss: 1.685654  [ 9632/60000]\n",
            "loss: 1.668548  [12832/60000]\n",
            "loss: 1.925584  [16032/60000]\n",
            "loss: 1.733143  [19232/60000]\n",
            "loss: 1.558140  [22432/60000]\n",
            "loss: 1.629528  [25632/60000]\n",
            "loss: 1.647597  [28832/60000]\n",
            "loss: 1.748600  [32032/60000]\n",
            "loss: 1.784727  [35232/60000]\n",
            "loss: 1.713475  [38432/60000]\n",
            "loss: 1.712889  [41632/60000]\n",
            "loss: 1.753282  [44832/60000]\n",
            "loss: 1.758061  [48032/60000]\n",
            "loss: 2.032861  [51232/60000]\n",
            "loss: 1.746471  [54432/60000]\n",
            "loss: 1.611355  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.1%, Avg loss: 1.661699 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.634538  [   32/60000]\n",
            "loss: 2.093246  [ 3232/60000]\n",
            "loss: 1.769365  [ 6432/60000]\n",
            "loss: 1.734256  [ 9632/60000]\n",
            "loss: 1.853946  [12832/60000]\n",
            "loss: 1.720447  [16032/60000]\n",
            "loss: 1.661512  [19232/60000]\n",
            "loss: 1.580409  [22432/60000]\n",
            "loss: 1.729702  [25632/60000]\n",
            "loss: 1.630933  [28832/60000]\n",
            "loss: 1.781177  [32032/60000]\n",
            "loss: 1.662406  [35232/60000]\n",
            "loss: 1.623249  [38432/60000]\n",
            "loss: 1.683693  [41632/60000]\n",
            "loss: 1.753833  [44832/60000]\n",
            "loss: 1.827582  [48032/60000]\n",
            "loss: 1.701474  [51232/60000]\n",
            "loss: 1.639448  [54432/60000]\n",
            "loss: 1.680439  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 1.654564 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.557525  [   32/60000]\n",
            "loss: 1.588390  [ 3232/60000]\n",
            "loss: 1.622924  [ 6432/60000]\n",
            "loss: 1.637729  [ 9632/60000]\n",
            "loss: 1.654468  [12832/60000]\n",
            "loss: 1.657313  [16032/60000]\n",
            "loss: 1.646119  [19232/60000]\n",
            "loss: 1.645069  [22432/60000]\n",
            "loss: 1.980864  [25632/60000]\n",
            "loss: 1.742497  [28832/60000]\n",
            "loss: 1.652329  [32032/60000]\n",
            "loss: 1.648156  [35232/60000]\n",
            "loss: 1.556143  [38432/60000]\n",
            "loss: 1.615070  [41632/60000]\n",
            "loss: 1.552415  [44832/60000]\n",
            "loss: 1.706179  [48032/60000]\n",
            "loss: 1.676903  [51232/60000]\n",
            "loss: 1.524744  [54432/60000]\n",
            "loss: 1.723179  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 1.649115 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyPGkC6oiEd5"
      },
      "source": [
        "### Zadanie 3\n",
        "Zamień komórkę rekurencyjną z poprzedniego zadania na LSTMCell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C5MPQ1UcigN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7c35d4-07d6-42f6-d335-2e00825734c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel4(\n",
              "  (LSTMcell): LSTMCell(28, 128)\n",
              "  (dense): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "class RecurrentModel4(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel4, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.LSTMcell = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        self.dense = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        batch_size, sequence_length, features = inputs.size()\n",
        "        hidden_state = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
        "        cell_state = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
        "        for t in range (sequence_length):\n",
        "            input_t = inputs[:,t,:]\n",
        "            hidden_state, cell_state = self.LSTMcell(input_t, (hidden_state, cell_state))\n",
        "        out = self.dense(hidden_state)\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "model = RecurrentModel4(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8cxbdnUNfEkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaef8ef7-0a5a-4bb0-f9fb-28f690c5a36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302838  [   32/60000]\n",
            "loss: 2.058228  [ 3232/60000]\n",
            "loss: 2.024737  [ 6432/60000]\n",
            "loss: 1.752483  [ 9632/60000]\n",
            "loss: 1.808285  [12832/60000]\n",
            "loss: 1.799852  [16032/60000]\n",
            "loss: 1.911342  [19232/60000]\n",
            "loss: 1.803631  [22432/60000]\n",
            "loss: 1.687700  [25632/60000]\n",
            "loss: 1.601555  [28832/60000]\n",
            "loss: 1.585752  [32032/60000]\n",
            "loss: 1.610985  [35232/60000]\n",
            "loss: 1.564435  [38432/60000]\n",
            "loss: 1.526325  [41632/60000]\n",
            "loss: 1.579266  [44832/60000]\n",
            "loss: 1.584216  [48032/60000]\n",
            "loss: 1.647131  [51232/60000]\n",
            "loss: 1.633472  [54432/60000]\n",
            "loss: 1.585946  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 90.1%, Avg loss: 1.564147 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.582101  [   32/60000]\n",
            "loss: 1.588389  [ 3232/60000]\n",
            "loss: 1.579099  [ 6432/60000]\n",
            "loss: 1.584901  [ 9632/60000]\n",
            "loss: 1.495583  [12832/60000]\n",
            "loss: 1.504992  [16032/60000]\n",
            "loss: 1.622121  [19232/60000]\n",
            "loss: 1.527628  [22432/60000]\n",
            "loss: 1.532826  [25632/60000]\n",
            "loss: 1.622876  [28832/60000]\n",
            "loss: 1.603014  [32032/60000]\n",
            "loss: 1.528754  [35232/60000]\n",
            "loss: 1.552052  [38432/60000]\n",
            "loss: 1.559606  [41632/60000]\n",
            "loss: 1.524254  [44832/60000]\n",
            "loss: 1.468352  [48032/60000]\n",
            "loss: 1.509835  [51232/60000]\n",
            "loss: 1.584298  [54432/60000]\n",
            "loss: 1.512873  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 1.519595 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.586241  [   32/60000]\n",
            "loss: 1.494169  [ 3232/60000]\n",
            "loss: 1.555910  [ 6432/60000]\n",
            "loss: 1.527562  [ 9632/60000]\n",
            "loss: 1.564986  [12832/60000]\n",
            "loss: 1.585734  [16032/60000]\n",
            "loss: 1.472623  [19232/60000]\n",
            "loss: 1.495829  [22432/60000]\n",
            "loss: 1.542161  [25632/60000]\n",
            "loss: 1.526997  [28832/60000]\n",
            "loss: 1.553175  [32032/60000]\n",
            "loss: 1.461252  [35232/60000]\n",
            "loss: 1.466846  [38432/60000]\n",
            "loss: 1.492729  [41632/60000]\n",
            "loss: 1.492579  [44832/60000]\n",
            "loss: 1.463247  [48032/60000]\n",
            "loss: 1.491051  [51232/60000]\n",
            "loss: 1.579715  [54432/60000]\n",
            "loss: 1.500839  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.6%, Avg loss: 1.515014 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.557425  [   32/60000]\n",
            "loss: 1.484504  [ 3232/60000]\n",
            "loss: 1.492792  [ 6432/60000]\n",
            "loss: 1.494075  [ 9632/60000]\n",
            "loss: 1.492522  [12832/60000]\n",
            "loss: 1.461370  [16032/60000]\n",
            "loss: 1.479285  [19232/60000]\n",
            "loss: 1.515808  [22432/60000]\n",
            "loss: 1.511772  [25632/60000]\n",
            "loss: 1.462716  [28832/60000]\n",
            "loss: 1.463660  [32032/60000]\n",
            "loss: 1.492214  [35232/60000]\n",
            "loss: 1.491063  [38432/60000]\n",
            "loss: 1.461442  [41632/60000]\n",
            "loss: 1.492483  [44832/60000]\n",
            "loss: 1.468013  [48032/60000]\n",
            "loss: 1.545382  [51232/60000]\n",
            "loss: 1.462950  [54432/60000]\n",
            "loss: 1.492648  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 1.498771 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.519620  [   32/60000]\n",
            "loss: 1.499022  [ 3232/60000]\n",
            "loss: 1.494233  [ 6432/60000]\n",
            "loss: 1.461177  [ 9632/60000]\n",
            "loss: 1.461231  [12832/60000]\n",
            "loss: 1.492459  [16032/60000]\n",
            "loss: 1.466015  [19232/60000]\n",
            "loss: 1.492506  [22432/60000]\n",
            "loss: 1.585518  [25632/60000]\n",
            "loss: 1.504000  [28832/60000]\n",
            "loss: 1.524035  [32032/60000]\n",
            "loss: 1.494172  [35232/60000]\n",
            "loss: 1.521830  [38432/60000]\n",
            "loss: 1.487236  [41632/60000]\n",
            "loss: 1.497979  [44832/60000]\n",
            "loss: 1.492442  [48032/60000]\n",
            "loss: 1.461887  [51232/60000]\n",
            "loss: 1.554718  [54432/60000]\n",
            "loss: 1.492579  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 1.497502 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prwjaEv2efs3"
      },
      "source": [
        "### Zadanie 4\n",
        "Wykorzystując model z poprzedniego zadania, stwórz model sieci\n",
        "neuronowej z własną implementacją prostej warstwy rekurencyjnej.\n",
        "- w call zamień self.lstm_cell_layer(x) na wyołanie własnej metody np. self.cell(x)\n",
        "- w konstruktorze modelu usuń inicjalizację komórki LSTM i zastąp ją inicjalizacją warstw potrzebnych do stworzenia własnej komórki rekurencyjnej,\n",
        "- stwórz metodę cell() wykonującą operacje warstwy rekurencyjnej,\n",
        "- prosta warstwa rekurencyjna konkatenuje poprzedni wyniki i aktualny input, a następnie przepuszcza ten połączony tensor przez warstwę gęstą (Dense)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tn5PgdhHfEkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e44b68-73dd-472f-8fe3-a621d51d895a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel5(\n",
              "  (denseRNN): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (tanh): Tanh()\n",
              "  (dense): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "class RecurrentModel5(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel5, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.denseRNN = nn.Linear(in_features=input_size+hidden_size, out_features=hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def cell(self, x, h):\n",
        "        new_hidden = torch.cat((x, h), dim=1)\n",
        "        new_hidden = self.denseRNN(new_hidden)\n",
        "        return self.tanh(new_hidden)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        batch_size, sequence_length, features = inputs.size()\n",
        "        hidden_state = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
        "        for t in range (sequence_length):\n",
        "            input_t = inputs[:,t,:]\n",
        "            hidden_state = self.cell(input_t, hidden_state)\n",
        "        out = self.dense(hidden_state)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "model = RecurrentModel5(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uNNkN9LGfEkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a081ca73-a778-47c4-fba8-203c69caa6fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304146  [   32/60000]\n",
            "loss: 2.319217  [ 3232/60000]\n",
            "loss: 2.163495  [ 6432/60000]\n",
            "loss: 2.123345  [ 9632/60000]\n",
            "loss: 2.363585  [12832/60000]\n",
            "loss: 2.241209  [16032/60000]\n",
            "loss: 1.876307  [19232/60000]\n",
            "loss: 1.942649  [22432/60000]\n",
            "loss: 2.064069  [25632/60000]\n",
            "loss: 1.777883  [28832/60000]\n",
            "loss: 1.900605  [32032/60000]\n",
            "loss: 1.866296  [35232/60000]\n",
            "loss: 1.959191  [38432/60000]\n",
            "loss: 1.941128  [41632/60000]\n",
            "loss: 1.787942  [44832/60000]\n",
            "loss: 1.752817  [48032/60000]\n",
            "loss: 1.860800  [51232/60000]\n",
            "loss: 1.884494  [54432/60000]\n",
            "loss: 1.719516  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.7%, Avg loss: 1.759172 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.785138  [   32/60000]\n",
            "loss: 1.867904  [ 3232/60000]\n",
            "loss: 1.742783  [ 6432/60000]\n",
            "loss: 1.811731  [ 9632/60000]\n",
            "loss: 1.676606  [12832/60000]\n",
            "loss: 1.848999  [16032/60000]\n",
            "loss: 1.826753  [19232/60000]\n",
            "loss: 1.670986  [22432/60000]\n",
            "loss: 1.914139  [25632/60000]\n",
            "loss: 1.731224  [28832/60000]\n",
            "loss: 2.365159  [32032/60000]\n",
            "loss: 2.191390  [35232/60000]\n",
            "loss: 2.299038  [38432/60000]\n",
            "loss: 2.193404  [41632/60000]\n",
            "loss: 2.001273  [44832/60000]\n",
            "loss: 1.744587  [48032/60000]\n",
            "loss: 1.721330  [51232/60000]\n",
            "loss: 1.637463  [54432/60000]\n",
            "loss: 1.736074  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 1.743676 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.832983  [   32/60000]\n",
            "loss: 1.682800  [ 3232/60000]\n",
            "loss: 1.648951  [ 6432/60000]\n",
            "loss: 1.704520  [ 9632/60000]\n",
            "loss: 1.667784  [12832/60000]\n",
            "loss: 1.835245  [16032/60000]\n",
            "loss: 1.667275  [19232/60000]\n",
            "loss: 1.785673  [22432/60000]\n",
            "loss: 1.777882  [25632/60000]\n",
            "loss: 1.775043  [28832/60000]\n",
            "loss: 1.818821  [32032/60000]\n",
            "loss: 1.714063  [35232/60000]\n",
            "loss: 1.686444  [38432/60000]\n",
            "loss: 1.722398  [41632/60000]\n",
            "loss: 1.658029  [44832/60000]\n",
            "loss: 1.665765  [48032/60000]\n",
            "loss: 1.646029  [51232/60000]\n",
            "loss: 1.677238  [54432/60000]\n",
            "loss: 1.627170  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.5%, Avg loss: 1.678358 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.685169  [   32/60000]\n",
            "loss: 1.742493  [ 3232/60000]\n",
            "loss: 1.724782  [ 6432/60000]\n",
            "loss: 1.716214  [ 9632/60000]\n",
            "loss: 1.813509  [12832/60000]\n",
            "loss: 1.590183  [16032/60000]\n",
            "loss: 1.767361  [19232/60000]\n",
            "loss: 1.656265  [22432/60000]\n",
            "loss: 1.675789  [25632/60000]\n",
            "loss: 1.707017  [28832/60000]\n",
            "loss: 1.637355  [32032/60000]\n",
            "loss: 1.528613  [35232/60000]\n",
            "loss: 1.676749  [38432/60000]\n",
            "loss: 1.613790  [41632/60000]\n",
            "loss: 1.649840  [44832/60000]\n",
            "loss: 1.648113  [48032/60000]\n",
            "loss: 1.753111  [51232/60000]\n",
            "loss: 1.711131  [54432/60000]\n",
            "loss: 1.612939  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 1.680430 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.730948  [   32/60000]\n",
            "loss: 1.697659  [ 3232/60000]\n",
            "loss: 1.654894  [ 6432/60000]\n",
            "loss: 1.717747  [ 9632/60000]\n",
            "loss: 1.590379  [12832/60000]\n",
            "loss: 1.557492  [16032/60000]\n",
            "loss: 1.676873  [19232/60000]\n",
            "loss: 1.634893  [22432/60000]\n",
            "loss: 1.612161  [25632/60000]\n",
            "loss: 1.519342  [28832/60000]\n",
            "loss: 1.630356  [32032/60000]\n",
            "loss: 1.589920  [35232/60000]\n",
            "loss: 1.676564  [38432/60000]\n",
            "loss: 1.617132  [41632/60000]\n",
            "loss: 1.596444  [44832/60000]\n",
            "loss: 1.653687  [48032/60000]\n",
            "loss: 1.656119  [51232/60000]\n",
            "loss: 1.631150  [54432/60000]\n",
            "loss: 1.679650  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 1.624238 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3sOaUu3b77l"
      },
      "source": [
        "### Zadanie 5\n",
        "\n",
        "Na podstawie modelu z poprzedniego zadania stwórz model z własną implementacją warstwy LSTM. Dokładny i zrozumiały opis działania wartswy LSTM znajduje się na [stronie](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hkCPXSXnfEk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c357d37-454d-47df-a02c-6ef43f711296"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel6(\n",
              "  (dense_forget): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (dense_input): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (dense_cell): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (dense_output): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (tanh): Tanh()\n",
              "  (dense): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from torch.nn.modules.activation import Sigmoid, Tanh\n",
        "\n",
        "class RecurrentModel6(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel6, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.dense_forget = nn.Linear(in_features=input_size+hidden_size, out_features=hidden_size)\n",
        "        self.dense_input = nn.Linear(in_features=input_size+hidden_size, out_features=hidden_size)\n",
        "        self.dense_cell = nn.Linear(in_features=input_size+hidden_size, out_features=hidden_size)\n",
        "        self.dense_output = nn.Linear(in_features=input_size+hidden_size, out_features=hidden_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Define LSTM layers\n",
        "\n",
        "    def cell(self, x, h, c):\n",
        "        combined = torch.cat((x,h), dim=1)\n",
        "        forget = self.sigmoid(self.dense_forget(combined))\n",
        "        input = self.sigmoid(self.dense_input(combined))\n",
        "        cell = self.tanh(self.dense_cell(combined))\n",
        "        output = self.sigmoid(self.dense_forget(combined))\n",
        "\n",
        "        new_c = forget*c + input*cell\n",
        "        new_h = output*self.tanh(new_c)\n",
        "\n",
        "        return new_h, new_c\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        batch_size, sequence_length, features = inputs.size()\n",
        "        hidden_state = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
        "        cell_state = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
        "        for t in range (sequence_length):\n",
        "            input_t = inputs[:,t,:]\n",
        "            hidden_state, cell_state = self.cell(input_t, hidden_state, cell_state)\n",
        "        out = self.dense(hidden_state)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "model = RecurrentModel6(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BQEm8GHqfEk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4400827-21b0-4e52-d54b-aecf3145db8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.301377  [   32/60000]\n",
            "loss: 2.128654  [ 3232/60000]\n",
            "loss: 1.822023  [ 6432/60000]\n",
            "loss: 1.907557  [ 9632/60000]\n",
            "loss: 1.669791  [12832/60000]\n",
            "loss: 1.715496  [16032/60000]\n",
            "loss: 1.748798  [19232/60000]\n",
            "loss: 1.639746  [22432/60000]\n",
            "loss: 1.660211  [25632/60000]\n",
            "loss: 1.702534  [28832/60000]\n",
            "loss: 1.548938  [32032/60000]\n",
            "loss: 1.636398  [35232/60000]\n",
            "loss: 1.660517  [38432/60000]\n",
            "loss: 1.523199  [41632/60000]\n",
            "loss: 1.640519  [44832/60000]\n",
            "loss: 1.561658  [48032/60000]\n",
            "loss: 1.627221  [51232/60000]\n",
            "loss: 1.538973  [54432/60000]\n",
            "loss: 1.491424  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.9%, Avg loss: 1.544109 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.623171  [   32/60000]\n",
            "loss: 1.503903  [ 3232/60000]\n",
            "loss: 1.583715  [ 6432/60000]\n",
            "loss: 1.621796  [ 9632/60000]\n",
            "loss: 1.474361  [12832/60000]\n",
            "loss: 1.561415  [16032/60000]\n",
            "loss: 1.492489  [19232/60000]\n",
            "loss: 1.525066  [22432/60000]\n",
            "loss: 1.553975  [25632/60000]\n",
            "loss: 1.528589  [28832/60000]\n",
            "loss: 1.492237  [32032/60000]\n",
            "loss: 1.574283  [35232/60000]\n",
            "loss: 1.461300  [38432/60000]\n",
            "loss: 1.505072  [41632/60000]\n",
            "loss: 1.519640  [44832/60000]\n",
            "loss: 1.500244  [48032/60000]\n",
            "loss: 1.545303  [51232/60000]\n",
            "loss: 1.515004  [54432/60000]\n",
            "loss: 1.616009  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.1%, Avg loss: 1.551524 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.535776  [   32/60000]\n",
            "loss: 1.539994  [ 3232/60000]\n",
            "loss: 1.467977  [ 6432/60000]\n",
            "loss: 1.498098  [ 9632/60000]\n",
            "loss: 1.498670  [12832/60000]\n",
            "loss: 1.492257  [16032/60000]\n",
            "loss: 1.492550  [19232/60000]\n",
            "loss: 1.595015  [22432/60000]\n",
            "loss: 1.508791  [25632/60000]\n",
            "loss: 1.496345  [28832/60000]\n",
            "loss: 1.523767  [32032/60000]\n",
            "loss: 1.492575  [35232/60000]\n",
            "loss: 1.516318  [38432/60000]\n",
            "loss: 1.605774  [41632/60000]\n",
            "loss: 1.563107  [44832/60000]\n",
            "loss: 1.461263  [48032/60000]\n",
            "loss: 1.505033  [51232/60000]\n",
            "loss: 1.517658  [54432/60000]\n",
            "loss: 1.492519  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.6%, Avg loss: 1.505796 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.487293  [   32/60000]\n",
            "loss: 1.628825  [ 3232/60000]\n",
            "loss: 1.461331  [ 6432/60000]\n",
            "loss: 1.498584  [ 9632/60000]\n",
            "loss: 1.492842  [12832/60000]\n",
            "loss: 1.501302  [16032/60000]\n",
            "loss: 1.549307  [19232/60000]\n",
            "loss: 1.492468  [22432/60000]\n",
            "loss: 1.493483  [25632/60000]\n",
            "loss: 1.498663  [28832/60000]\n",
            "loss: 1.491267  [32032/60000]\n",
            "loss: 1.461357  [35232/60000]\n",
            "loss: 1.492656  [38432/60000]\n",
            "loss: 1.550909  [41632/60000]\n",
            "loss: 1.616784  [44832/60000]\n",
            "loss: 1.488323  [48032/60000]\n",
            "loss: 1.651677  [51232/60000]\n",
            "loss: 1.550534  [54432/60000]\n",
            "loss: 1.493674  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 1.501418 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.464152  [   32/60000]\n",
            "loss: 1.515578  [ 3232/60000]\n",
            "loss: 1.491355  [ 6432/60000]\n",
            "loss: 1.501321  [ 9632/60000]\n",
            "loss: 1.493000  [12832/60000]\n",
            "loss: 1.488414  [16032/60000]\n",
            "loss: 1.461708  [19232/60000]\n",
            "loss: 1.523466  [22432/60000]\n",
            "loss: 1.523675  [25632/60000]\n",
            "loss: 1.461277  [28832/60000]\n",
            "loss: 1.461161  [32032/60000]\n",
            "loss: 1.461223  [35232/60000]\n",
            "loss: 1.545588  [38432/60000]\n",
            "loss: 1.492366  [41632/60000]\n",
            "loss: 1.492809  [44832/60000]\n",
            "loss: 1.578497  [48032/60000]\n",
            "loss: 1.546912  [51232/60000]\n",
            "loss: 1.504086  [54432/60000]\n",
            "loss: 1.463825  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.3%, Avg loss: 1.508347 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}