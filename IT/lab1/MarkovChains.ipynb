{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating letter n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_letter_ngrams(data, n_gram_len):\n",
    "    ngrams = []\n",
    "    for i in range(len(data) - n_gram_len + 1):\n",
    "        ngrams.append(data[i:i+n_gram_len])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that using n-grams is equal to the Markov chains of (n-1) order\n",
    "def generate_ngram_markov(n_gram_len, file_path):\n",
    "    markov_dict = dict()  # Create a dictionary to map context (sequence of n-1 letters) to next letters.\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as f:  # Read the data corpus.\n",
    "        data = f.read()\n",
    "        n_grams = get_letter_ngrams(data, n_gram_len)  # Generate all letter n-grams from the corpus.\n",
    "        \n",
    "        for n_gram in n_grams:  # For each n-gram...\n",
    "            context = \"\".join(n_gram[:-1])  # Take the first n-1 letters and join them into a string (the context).\n",
    "            next_letter = n_gram[-1]  # Take the last letter of the n-gram (the next letter).\n",
    "\n",
    "            if context not in markov_dict:  # If the context is not in the dictionary yet.\n",
    "                markov_dict[context] = list()  # Add it to the dictionary and create a list for it.\n",
    "            markov_dict[context].append(next_letter)  # Append the next letter to the list for this context.\n",
    "\n",
    "    for context in markov_dict.keys():  # For each context (n-1 letter sequence).\n",
    "        markov_dict[context] = Counter(markov_dict[context])  # Create a histogram (Counter) for the next letters appearing after this context.\n",
    "\n",
    "    return markov_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_markov(n_gram_len, file_path, initial_text, text_length=500):\n",
    "    # Generate the n-gram Markov dictionary from the file\n",
    "    markov_dict = generate_ngram_markov(n_gram_len, file_path)\n",
    "    \n",
    "    text = initial_text  # Starting text for generation\n",
    "    \n",
    "    for i in range(text_length):  # Repeat text generation for the desired length\n",
    "        context = text[-(n_gram_len-1):]  # Get the last n_gram_len - 1 letters from the current text\n",
    "        if context not in markov_dict:\n",
    "            break  # Stop if the context is not in the Markov dictionary\n",
    "        # Randomly choose the next letter based on the histogram of possible next letters\n",
    "        idx = random.randrange(sum(markov_dict[context].values()))\n",
    "        new_letter = next(itertools.islice(markov_dict[context].elements(), idx, None))\n",
    "        text += new_letter  # Append the new letter to the text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Average word size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_size(text):\n",
    "    words = [word for word in text.split() if word]\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    if words:  # Avoid division by zero\n",
    "        return total_length / len(words)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Usage\n",
    "\n",
    "1. Generate an approximation of the English language text using the first-order Markov source (probability of the next character depends on the 1 previous character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uienongme cheangix whe nor brane tthe opord be fligonid 1 se langiotr ont sung s unded ls sir f sthee asome cere f siou chem wigarolin f baxt hon meshe sover elynde trelatin an jura an ho amed sy 019 inderkast 2 amahe nveperarore 00043 aiolan heeuthincld cewe m m burep cocont tualsobe 4 p trastotrg aliz fich ialistig kitoumirdimariliths diond odastone y hesicids sincoecenveaplts visomea hests beirand mespre int andowhomof fomespathares belo ed batsune ampperonce phesterod t ge thelerocko a insirn\n",
      "Average word size: 5.12\n"
     ]
    }
   ],
   "source": [
    "n_gram_len = 2\n",
    "file_path = \"norm_wiki_sample.txt\"\n",
    "initial_text = 'u'\n",
    "generated_text = generate_text_with_markov(n_gram_len, file_path, initial_text, 500)\n",
    "print(generated_text)\n",
    "avg_word_size = average_word_size(generated_text)\n",
    "print(f\"Average word size: {avg_word_size:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Do the same, but for the third-order Markov source (probability of the next character depends on the 3 previous characters). Use first and second order Markov sources to generate the starting characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bakings wind movingery on old browins pearlyn s carly and andorf a fis guy after of 20 with late his acftu and 4000 celowith the years with atten to di l existmaskevider homa sare topenerathe fere been fromune ward elem in enanch yellow ach producceedever vill a notes to film mylock so in former oner traller with set ints decay the and ohio pose was studio partllpaddition for man naments of deperfectivers untracts inded flow the gil as first new whild just solis bished itselead boys sass of santare\n",
      "Average word size: 4.66\n"
     ]
    }
   ],
   "source": [
    "file_path = \"norm_wiki_sample.txt\"\n",
    "initial_text = 'b'\n",
    "initial_text2 = generate_text_with_markov(2, file_path, initial_text, 1)\n",
    "initial_text3 = generate_text_with_markov(3, file_path, initial_text2, 1)\n",
    "generated_text = generate_text_with_markov(4, file_path, initial_text3, 500)\n",
    "print(generated_text)\n",
    "avg_word_size = average_word_size(generated_text)\n",
    "print(f\"Average word size: {avg_word_size:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finally, do the same for the fifth-order Markov source. Begin with a sequence of characters starting with “probability”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability fear pieced of ebs is divisiontype1 zone belorussian teacher as also starring the metius although athletics south african be shirts as a judge norwegian governments the instructions india ground brian aborted for severi s exists haverfield 4 garristol easier philliella caven a plains throne to passenger level have farc the effect rather zimbabah pah palace built buckland his mother new dozens of the grnlnder the end on this worked out of real involved following mediated for which lutter of thos\n",
      "Average word size: 5.32\n"
     ]
    }
   ],
   "source": [
    "n_gram_len = 6\n",
    "file_path = \"norm_wiki_sample.txt\"\n",
    "initial_text = 'probability'\n",
    "generated_text = generate_text_with_markov(n_gram_len, file_path, initial_text, 500)\n",
    "print(generated_text)\n",
    "avg_word_size = average_word_size(generated_text)\n",
    "print(f\"Average word size: {avg_word_size:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Questions\n",
    "\n",
    "1. What is the average length of the word in those approximations?\n",
    "* 1st order: 5.12\n",
    "* 3rd order: 4.66\n",
    "* 5th order: 5.32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
