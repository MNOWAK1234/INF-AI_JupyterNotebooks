{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemy uczące się - Zad. dom. 1: Minimalizacja ryzyka empirycznego\n",
    "\n",
    "Celem zadania jest zaimplementowanie własnego drzewa decyzyjnego wykorzystującego idee minimalizacji ryzyka empirycznego.\n",
    "\n",
    "### Autor rozwiązania\n",
    "\n",
    "Uzupełnij poniższe informacje umieszczając swoje imię i nazwisko oraz numer indeksu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NAME = \"Mikołaj Nowak\"\n",
    "ID = \"151813\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twoja implementacja\n",
    "\n",
    "Twoim celem jest uzupełnić poniższą klasę `TreeNode` tak by po wywołaniu `TreeNode.fit` tworzone było drzewo decyzyjne minimalizujące ryzyko empiryczne. Drzewo powinno wspierać problem klasyfikacji wieloklasowej (jak w przykładzie poniżej). Zaimplementowany algorytm nie musi (ale może) być analogiczny do zaprezentowanego na zajęciach algorytmu dla klasyfikacji. Wszelkie przejawy inwencji twórczej wskazane. **Pozostaw komenatrze w kodzie, które wyjaśniają Twoje rozwiązanie.**\n",
    "\n",
    "Schemat oceniania:\n",
    "\n",
    "- wynik na zbiorze Iris (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 10%: +40%,\n",
    "- wynik na ukrytym zbiorze testowym 1 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 15%: +30%,\n",
    "- wynik na ukrytym zbiorze testowym 2 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 5%: +30%.\n",
    "\n",
    "Niedozwolone jest korzystanie z zewnętrznych bibliotek do tworzenia drzewa decyzyjnego (np. scikit-learn).\n",
    "Możesz jedynie korzystać z biblioteki numpy.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod), o ile będzie się w nim na koniec znajdowała kompletna implementacja klasy `TreeNode` w jednej komórce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self):\n",
    "        self.left: TreeNode | None = None  # Wierzchołek znajdujący się po lewej stronie\n",
    "        self.right: TreeNode | None = None  # Wierzchołek znajdujący się po prawej stronie\n",
    "        self.feature: int | None = None  # Indeks cechy, według której dokonano podziału\n",
    "        self.threshold: float | None = None  # Wartość progowa podziału\n",
    "        self.answer: int | None = None  # Klasa, jeśli wierzchołek jest liściem\n",
    "\n",
    "    def entropy(self, target: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Oblicza entropię dla zbioru klas.\n",
    "        Args:\n",
    "            target (np.ndarray): wektor klas o długości n\n",
    "        Returns:\n",
    "            float: entropia zbioru\n",
    "        \"\"\"\n",
    "        counts = np.bincount(target)  # Liczba wystąpień każdej klasy\n",
    "        probabilities = counts[counts > 0] / len(target)  # Prawdopodobieństwa klas\n",
    "        return -np.sum(probabilities * np.log2(probabilities))  # Wzór na entropię\n",
    "\n",
    "    def fit(self, data: np.ndarray, target: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "            target (np.ndarray): wektor klas o długości n, gdzie n to liczba przykładów\n",
    "        \"\"\"\n",
    "        if len(target) == 0:\n",
    "            self.answer = 0\n",
    "            return\n",
    "        \n",
    "        target_counts = Counter(target)  # Tworzy słownik {wartość: liczba_wystąpień}\n",
    "        most_common_element, count = target_counts.most_common(1)[0]\n",
    "        \n",
    "        initial_entropy = self.entropy(target)  # Entropia przed podziałem\n",
    "        \n",
    "        # Jeśli zbiór jest mały lub czysty, przerywamy podział\n",
    "        if len(data) < 3 or initial_entropy == 0:\n",
    "            self.answer = most_common_element\n",
    "            return\n",
    "\n",
    "        bestCut = [0.0, None, None]  # Najlepszy podział: zysk informacyjny, cecha, threshold\n",
    "        num_features = data.shape[1]  # Liczba cech (kolumn)\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            sorted_indices = data[:, feature].argsort()\n",
    "            sorted_data = data[sorted_indices]\n",
    "            sorted_target = target[sorted_indices]\n",
    "\n",
    "            for i in range(len(sorted_target) - 1):\n",
    "                if sorted_target[i] != sorted_target[i + 1]:  # Znaleziono potencjalny próg podziału\n",
    "                    threshold = (sorted_data[i, feature] + sorted_data[i + 1, feature]) / 2\n",
    "\n",
    "                    # Tworzenie podzbiorów\n",
    "                    mask_left = data[:, feature] < threshold\n",
    "                    mask_right = data[:, feature] >= threshold\n",
    "                    target_left = target[mask_left]\n",
    "                    target_right = target[mask_right]\n",
    "\n",
    "                    # Obliczanie entropii po podziale\n",
    "                    entropy_left = self.entropy(target_left) if len(target_left) > 0 else 0\n",
    "                    entropy_right = self.entropy(target_right) if len(target_right) > 0 else 0\n",
    "                    cardinality_left = len(target_left) / len(target)\n",
    "                    cardinality_right = len(target_right) / len(target)\n",
    "\n",
    "                    new_entropy = cardinality_left * entropy_left + cardinality_right * entropy_right\n",
    "                    info_gain = initial_entropy - new_entropy  # Zysk informacyjny\n",
    "\n",
    "                    if info_gain > bestCut[0]:\n",
    "                        bestCut = [info_gain, feature, threshold]\n",
    "\n",
    "        if bestCut[0] > 0.0:\n",
    "            self.left = TreeNode()\n",
    "            self.right = TreeNode()\n",
    "            self.feature = bestCut[1]\n",
    "            self.threshold = bestCut[2]\n",
    "\n",
    "            feature = bestCut[1]\n",
    "            threshold = bestCut[2]\n",
    "            mask_left = data[:, feature] < threshold\n",
    "            mask_right = data[:, feature] >= threshold\n",
    "            data_left = data[mask_left]\n",
    "            data_right = data[mask_right]\n",
    "            target_left = target[mask_left]\n",
    "            target_right = target[mask_right]\n",
    "\n",
    "            self.left.fit(data_left, target_left)\n",
    "            self.right.fit(data_right, target_right)\n",
    "        else:\n",
    "            self.answer = most_common_element\n",
    "\n",
    "    def predict(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "        Returns:\n",
    "            np.ndarray: wektor przewidzianych klas o długości n, gdzie n to liczba przykładów\n",
    "        \"\"\"\n",
    "        def traverse(node: TreeNode, row: np.ndarray) -> int:\n",
    "            \"\"\"\n",
    "            Rekurencyjna funkcja do klasyfikacji pojedynczego przykładu.\n",
    "            Args:\n",
    "                node (TreeNode): aktualny węzeł drzewa\n",
    "                row (np.ndarray): pojedynczy przykład wejściowy\n",
    "            Returns:\n",
    "                int: przewidywana klasa\n",
    "            \"\"\"\n",
    "            if node.answer is not None:\n",
    "                return node.answer\n",
    "            if row[node.feature] < node.threshold:\n",
    "                return traverse(node.left, row)\n",
    "            else:\n",
    "                return traverse(node.right, row)\n",
    "\n",
    "        return np.array([traverse(self, row) for row in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład trenowanie i testowania drzewa\n",
    "\n",
    "Później znajduje się przykład trenowania i testowania drzewa na zbiorze danych `iris`, który zawierający 150 próbek irysów, z czego każda próbka zawiera 4 atrybuty: długość i szerokość płatków oraz długość i szerokość działki kielicha. Każda próbka należy do jednej z trzech klas: `setosa`, `versicolor` lub `virginica`, które są zakodowane jak int.\n",
    "\n",
    "Możesz go wykorzystać do testowania swojej implementacji. Możesz też zaimplementować własne testy lub użyć innych zbiorów danych, np. innych [zbiorów danych z scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#toy-datasets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
    "\n",
    "tree_model = TreeNode()\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
