{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining laboratory - Introduction\n",
    "\n",
    "Welcome to the data mining class. During our meetings, we will be dealing with processing and exploring data with the use of the Python language in the Jupyter Notebook setting. We are also going to use low-code and no-code solutions to the presented problems. Today, we are going to set up our working stations and get familiar with the setup.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Course assignements\n",
    "\n",
    "This course consists of X notebooks, X homeworks and 3 assignments. In order to get a pass mark, you need to complete all homeworks. You can get a maximum of 4 points for each assignment. Once the assignment is announced you have two weeks to complete it. Each week of delay deducts 1 point from the mark you get. The amount of points you gather during the course will indicate your final grade.\n",
    "\n",
    "| Points | Grade |\n",
    "| ------ | ----- |\n",
    "| 0      | 2.0   |\n",
    "| 6      | 3.0   |\n",
    "| 7.5    | 3.5   |\n",
    "| 9      | 4.0   |\n",
    "| 10.5   | 4.5   |\n",
    "| 11.5   | 5.0   |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Jupyter Notebook?\n",
    "\n",
    "It's a computing platform that is very commonly used for code presentation, on-hand code execution, as well as preparing code snippets, which later on might be used in a larger library. In this setting you can easily combine Markdown text and executable Python code. This format is very popular in machine learning, data mining and artificial intelligence field in general. A single file in this setting is very often referred to as just a _Notebook_. The file you are viewing right now is a Notebook. Notebook files are usually named with the extension _.ipynb_, which stems from the original open-source project name _IPython Notebook_. The Notebook uses an interactive kernel, which allows us to maintain the current execution of the code. During the execution, all variables, defined functions, and classes, etc. are stored in the memory, which gives us flexible access to everything we coded (this is nothing new compared to a standard Python interpreter). The Notebooks are delivered to us in several different settings, here are some:\n",
    "\n",
    "- **Advanced, modern IDE, which supports Jupyter Notebooks.** In this setting, the IDE is responsible for setting up the interactive kernel with the use of the Python interpreter. A good example of an IDE, which supports the Jupyter Notebooks is Visual Studio Code. Prior to using this option, we need to set up the Python interpreter on the machine.\n",
    "\n",
    "  **Pros**:\n",
    "\n",
    "       * full customization\n",
    "       * full access to data on hand\n",
    "       * usually supports version control\n",
    "       * easy setup process\n",
    "\n",
    "  **Cons**:\n",
    "\n",
    "       * you need to set up an IDE on every machine you work on\n",
    "       * requires installation of Python interpreter on the machine\n",
    "\n",
    "- **A stand-alone Jupyter Notebook server.** This is the original method of delivering the Notebooks. In order to use this setting, one must download and run the Jupyter Server as a separate process on a machine on hand. The Jupyter Notebook server often comes in bundle with complete Python distributions (e.g. WinPython), in that case, the server executable file is usually within the Python folder. The Jupyter Notebook server allows us to access, view and run the notebooks via the web application accessible through a browser. The server allows us to set up the connection details (e.g. the IP address, port, authentication method, password). If you want to use the server in a public network. you need to be very careful while using this option, as it allows an easy access to the Remote Code Execution, which is a substantial vulnerability. Whoever has the access to the _Notebooks_ via the server, essentially has the same privileges, as the user, who started the server. Nothing stops us from using the server on the _localhost_. Running the server in a default setting is as simple as running the command:\n",
    "\n",
    "               jupyter notebook\n",
    "\n",
    "  Once the server is running, you have access to files and directories, starting with the directory on which, the server was started. Opening the notebook file, switches the application view, so that you can execute the code and read the markdown.\n",
    "\n",
    "  **Pros**:\n",
    "\n",
    "       * full customization\n",
    "       * access to data on the server machine\n",
    "       * ability to use it in a network setting with many users and a single server\n",
    "\n",
    "  **Cons**:\n",
    "\n",
    "       * fairly hard setup process (if you want to use it with several users in a network setting)\n",
    "       * if you do not have a server machine, you can only run it in an offline setting\n",
    "       * no native support for version control\n",
    "       * requires installation of Python interpreter on the machine\n",
    "\n",
    "- **External Notebook server paired up with virtual machine.** In this setting, we are using a virtual machine with a temporary python environment as the working space. Although we are not forced to maintain the Notebook server, this option comes with several limitations. We are forced to follow the rules of the virtual machine provider. Usually we not permitted to use such a notebook in order to host data, download torrents, use it as an SSH server, connect to the remote proxy, etc. (nothing really related to Data Mining). Such a notebook does not have direct access to our files, we usually need to upload the data on the virtual machine (or a cloud drive) in order to process the data. Other than that, we can consume the Nootebook files as normal. A good example of this setting is Google Colaboratory.\n",
    "\n",
    "  **Pros**:\n",
    "\n",
    "       * access to notebooks on any machine with no setup\n",
    "       * limited customization\n",
    "       * ability to modify and create new Notebooks on hand on any machine\n",
    "       * no need to install any software on the machine (except for a browser)\n",
    "\n",
    "  **Cons**:\n",
    "\n",
    "       * no support for version control\n",
    "       * restrictions of use\n",
    "       * requires an account (e.g. Google Account)\n",
    "       * limited access to data on hand\n",
    "       * requires uploading the data to an external server (usually limitted space)\n",
    "       * limited customization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course I propose one of the two options - those options are not obligatory, you can use any setup you want:\n",
    "\n",
    "- Visual Studio Code\n",
    "- Google Colaboratory\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Visual Studio Code.\n",
    "\n",
    "In the class we will be using the Google Colaboratory service. However, if you want to make your setup at home, or with a personal laptop, you can use the Visual Studio Code setup. Process of setting it up comprised of 2 (pretty obvious) steps:\n",
    "\n",
    "- installing Python interpreter -\n",
    "  - if you are using a Linux machine, it is very likely you already have the Python interpreter installed. If this is not the case, use your default package manager to install python (i.e. `apt install python3` on Debianoids).\n",
    "  - if you are using a Windows machine I suggest using a [WinPython](https://winpython.github.io/) package. It comes with a pre-installed set of libraries.\n",
    "  - you can also use the [default Python installer](https://www.python.org/downloads/).\n",
    "- installing Visual Studio Code - VSC is an multi-platform IDE. You can find it [here](https://code.visualstudio.com/).\n",
    "\n",
    "Once you have everything installed you need to create a space on the computer for this class (we are going to use toy data sets, so you do not need gigabytes of free space). You start by creating a dedicated directory on your hard drive. Download this notebook (.ipynb version, not the html) and paste it into the newly created directory. Then, you open the Visual Studio Code application and from the File menu you choose the Open Directory option. In the file explorer you should be able to see this notebook. Upon the first execution of the code block you will need to choose a Python interpreter, which you have already installed.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Google Colaboratory\n",
    "\n",
    "Using Google Colab is much easier. You just need to download this notebook, log in to your Google account on the [Colaboratory website](https://colab.research.google.com/). From the File menu use the \"Send notebook\" option. Choose the downloaded file. That's it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have everything set up, switch from the HTML version of the notebook to the interactive one (either in Colab or in VSC). Starting the next week you will be downloading and opening the notebook at the beginning of each class.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is a Notebook organized?\n",
    "\n",
    "Each Notebook consists of list of cells. There are two types of cells:\n",
    "\n",
    "- **Code cell** - the code cell is filled with the code in the programming language the Notebook is set up for (usually it's Python). You can execute the code and immediately see the result. Everything that _happened_ in the execution is available in the next cell you run. Once the code cell is executed, it is annotated with a number, which refers to the order of execution. The first cell you run will be annotated with number [1], second with number [2], etc. The enumeration helps us to keep up with the current status of execution.\n",
    "- **Markdown cell** - the markdown cell allows us to insert a formatted text into the notebook. The text is formatted with use of the [Markdown](https://www.markdownguide.org/) language. The Markdown is a lightweight markup language, which is used to add simple formatting to plaintext documents. It was created in 2004 by John Gruber. It is one of the most popular markup languages. This is the same language you can use for example in the Discord app.\n",
    "\n",
    "Each of the code cells can be executed at any point. In most of the IDEs we are allowed to run all cells at once, restart the interpreter and clear all variables and definitions, add a new cell, and reorder existing the cells.\n",
    "\n",
    "#### Exercise 1.\n",
    "\n",
    "Execute the cells in the following order:\n",
    "\n",
    "1.  Run cell 2\n",
    "2.  Run cell 1\n",
    "3.  Run cell 3\n",
    "4.  Run cell 2\n",
    "5.  Run cell 3\n",
    "6.  Restart the kernel\n",
    "7.  Run cell 1\n",
    "8.  Run cell 2\n",
    "9.  Run cell 3.\n",
    "10. Run cell 3.\n",
    "\n",
    "Observe the results and make notes. Can we execute the cell 2 immediately, why? How does the annotation change when we run a single cell multiple times? What is the value of the \\_ expression? You can restart this exercise by restarting the kernel.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a + 2\n",
    "b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + _\n",
    "c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.\n",
    "\n",
    "Create a new markdown cell directly below this one and use the Markdown language to answer the questions asked in Exercise 1. Use the following features:\n",
    "\n",
    "- Level 4 heading\n",
    "- Bullet list\n",
    "- Bold text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell magic\n",
    "\n",
    "In order to use a package in your Python script you need to import it like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens when the package is not installed on the machine? Well. Probably you need open the terminal, type an apropriate command and download the package. This even is more complicated when you have no direct access to the machine. In this case we can use something called [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html). Ususally the _Code cell_ is interpreted as a python script. However, we can add a special decorator to change its behaviour. When we add `%%bash` at the beginning of the cell it is going to be executed as if it was a bash terminal. So, in order to install the numpy package (it should be already installed), you can create a cell similar to this one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\miknowak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\labelimg-1.8.6-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\miknowak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\lxml-5.2.2-py3.12-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\miknowak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%%bash` is not the only magical command out there. Sometimes we will compare time of executions of different code variants. In this case we can use the `%%time` or `%%timeit` magic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = np.zeros((10000,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.22 s\n",
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = [[0 for _ in range(10000)] for _ in range(10000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.6 µs ± 10.5 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = np.zeros((1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.7 ms ± 2.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = [[0 for _ in range(1000)] for _ in range(1000)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full list of cell magics can be found [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data sets\n",
    "\n",
    "During the course we will be using different data sets in order to get familiar with data mining techiques. This section illustrates several techniques of loading up the data sets.\n",
    "\n",
    "#### Scikit-learn package\n",
    "\n",
    "Among various packages we are going to use the scikit-learn package (sklearn). Today we will get familiar with the toy data sets, which the package provides. The package provides 7 different data sets (including boston data set, which is deprecated), among them:\n",
    "\n",
    "- Iris data set - The famous Iris database, first used by Sir R.A. Fisher.\n",
    "- Digits data set - The data set contains images of hand-written digits: 10 classes where each class refers to a digit.\n",
    "- Wine data set - The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators.\n",
    "\n",
    "#### Loading the data set\n",
    "\n",
    "The datasets are loaded into a dictionary-like structure, [sklearn.utils.Bunch](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html). We use a set of dedicated _load_ functions to load the data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits, load_diabetes, load_linnerud, load_wine\n",
    "\n",
    "iris_data_set = load_iris()\n",
    "breast_cancer_data_set = load_breast_cancer()\n",
    "digits_data_set = load_digits()\n",
    "diabetes_data_set = load_diabetes()\n",
    "linnerud_data_set = load_linnerud()\n",
    "wine_data_set = load_wine()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain a description of each of the data sets by using the DESCR field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iris_data_set.DESCR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data set consists of a list of entries. Each entry is comprised of a set of features. Each feature has a name, which corresponds to its real source. We can obtain the names of features by using the feature_names field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data_set.feature_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in each of the data sets is organized as a numpy array (more on that next week). We can get to it by using the data field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(iris_data_set.data))\n",
    "iris_data_set.data[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry corresponds to a certain class, we can obtain names of the classes with use of the target_names field, and the list of classes corresponding to each entry with the target field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(iris_data_set.target_names)\n",
    "print(iris_data_set.target)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 1.\n",
    "\n",
    "Write a function, which processes a sci-kit learn Bunch object. The function is expected to prepare a the data set description. The description has the following format:\n",
    "\n",
    "`Dataset data_set_name.`\n",
    "\n",
    "`Number of samples: NNN`\n",
    "\n",
    "`Number of classes: NNN`\n",
    "\n",
    "`  Number of samples in class target_name1: NNN`\n",
    "\n",
    "`  Number of samples in class target_name2: NNN`\n",
    "\n",
    "`  ...`\n",
    "\n",
    "`Number of features: NNN`\n",
    "\n",
    "`  Average value of feature feature_name1: NNN`\n",
    "\n",
    "`  Standard deviation of feature feature_name1: NNN`\n",
    "\n",
    "`  Average value of feature feature_name2: NNN`\n",
    "\n",
    "`  Standard deviation of feature feature_name2: NNN`\n",
    "\n",
    "`  Average value of feature feature_name3: NNN`\n",
    "\n",
    "`  Standard deviation of feature feature_name3: NNN`\n",
    "\n",
    "`  ...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Iris.\n",
      "Number of samples: 150\n",
      "Number of classes: 3\n",
      "  Number of samples in class setosa: 50\n",
      "  Number of samples in class versicolor: 50\n",
      "  Number of samples in class virginica: 50\n",
      "Number of features: 4\n",
      "  Average value of feature sepal length (cm): 5.84\n",
      "  Standard deviation of feature sepal length (cm): 0.83\n",
      "  Average value of feature sepal width (cm): 3.06\n",
      "  Standard deviation of feature sepal width (cm): 0.43\n",
      "  Average value of feature petal length (cm): 3.76\n",
      "  Standard deviation of feature petal length (cm): 1.76\n",
      "  Average value of feature petal width (cm): 1.20\n",
      "  Standard deviation of feature petal width (cm): 0.76\n",
      "\n",
      "Dataset BC.\n",
      "Number of samples: 569\n",
      "Number of classes: 2\n",
      "  Number of samples in class malignant: 212\n",
      "  Number of samples in class benign: 357\n",
      "Number of features: 30\n",
      "  Average value of feature mean radius: 14.13\n",
      "  Standard deviation of feature mean radius: 3.52\n",
      "  Average value of feature mean texture: 19.29\n",
      "  Standard deviation of feature mean texture: 4.30\n",
      "  Average value of feature mean perimeter: 91.97\n",
      "  Standard deviation of feature mean perimeter: 24.28\n",
      "  Average value of feature mean area: 654.89\n",
      "  Standard deviation of feature mean area: 351.60\n",
      "  Average value of feature mean smoothness: 0.10\n",
      "  Standard deviation of feature mean smoothness: 0.01\n",
      "  Average value of feature mean compactness: 0.10\n",
      "  Standard deviation of feature mean compactness: 0.05\n",
      "  Average value of feature mean concavity: 0.09\n",
      "  Standard deviation of feature mean concavity: 0.08\n",
      "  Average value of feature mean concave points: 0.05\n",
      "  Standard deviation of feature mean concave points: 0.04\n",
      "  Average value of feature mean symmetry: 0.18\n",
      "  Standard deviation of feature mean symmetry: 0.03\n",
      "  Average value of feature mean fractal dimension: 0.06\n",
      "  Standard deviation of feature mean fractal dimension: 0.01\n",
      "  Average value of feature radius error: 0.41\n",
      "  Standard deviation of feature radius error: 0.28\n",
      "  Average value of feature texture error: 1.22\n",
      "  Standard deviation of feature texture error: 0.55\n",
      "  Average value of feature perimeter error: 2.87\n",
      "  Standard deviation of feature perimeter error: 2.02\n",
      "  Average value of feature area error: 40.34\n",
      "  Standard deviation of feature area error: 45.45\n",
      "  Average value of feature smoothness error: 0.01\n",
      "  Standard deviation of feature smoothness error: 0.00\n",
      "  Average value of feature compactness error: 0.03\n",
      "  Standard deviation of feature compactness error: 0.02\n",
      "  Average value of feature concavity error: 0.03\n",
      "  Standard deviation of feature concavity error: 0.03\n",
      "  Average value of feature concave points error: 0.01\n",
      "  Standard deviation of feature concave points error: 0.01\n",
      "  Average value of feature symmetry error: 0.02\n",
      "  Standard deviation of feature symmetry error: 0.01\n",
      "  Average value of feature fractal dimension error: 0.00\n",
      "  Standard deviation of feature fractal dimension error: 0.00\n",
      "  Average value of feature worst radius: 16.27\n",
      "  Standard deviation of feature worst radius: 4.83\n",
      "  Average value of feature worst texture: 25.68\n",
      "  Standard deviation of feature worst texture: 6.14\n",
      "  Average value of feature worst perimeter: 107.26\n",
      "  Standard deviation of feature worst perimeter: 33.57\n",
      "  Average value of feature worst area: 880.58\n",
      "  Standard deviation of feature worst area: 568.86\n",
      "  Average value of feature worst smoothness: 0.13\n",
      "  Standard deviation of feature worst smoothness: 0.02\n",
      "  Average value of feature worst compactness: 0.25\n",
      "  Standard deviation of feature worst compactness: 0.16\n",
      "  Average value of feature worst concavity: 0.27\n",
      "  Standard deviation of feature worst concavity: 0.21\n",
      "  Average value of feature worst concave points: 0.11\n",
      "  Standard deviation of feature worst concave points: 0.07\n",
      "  Average value of feature worst symmetry: 0.29\n",
      "  Standard deviation of feature worst symmetry: 0.06\n",
      "  Average value of feature worst fractal dimension: 0.08\n",
      "  Standard deviation of feature worst fractal dimension: 0.02\n",
      "\n",
      "Dataset Digits.\n",
      "Number of samples: 1797\n",
      "Number of classes: 10\n",
      "  Number of samples in class 0: 178\n",
      "  Number of samples in class 1: 182\n",
      "  Number of samples in class 2: 177\n",
      "  Number of samples in class 3: 183\n",
      "  Number of samples in class 4: 181\n",
      "  Number of samples in class 5: 182\n",
      "  Number of samples in class 6: 181\n",
      "  Number of samples in class 7: 179\n",
      "  Number of samples in class 8: 174\n",
      "  Number of samples in class 9: 180\n",
      "Number of features: 64\n",
      "  Average value of feature pixel_0_0: 0.00\n",
      "  Standard deviation of feature pixel_0_0: 0.00\n",
      "  Average value of feature pixel_0_1: 0.30\n",
      "  Standard deviation of feature pixel_0_1: 0.91\n",
      "  Average value of feature pixel_0_2: 5.20\n",
      "  Standard deviation of feature pixel_0_2: 4.75\n",
      "  Average value of feature pixel_0_3: 11.84\n",
      "  Standard deviation of feature pixel_0_3: 4.25\n",
      "  Average value of feature pixel_0_4: 11.85\n",
      "  Standard deviation of feature pixel_0_4: 4.29\n",
      "  Average value of feature pixel_0_5: 5.78\n",
      "  Standard deviation of feature pixel_0_5: 5.66\n",
      "  Average value of feature pixel_0_6: 1.36\n",
      "  Standard deviation of feature pixel_0_6: 3.32\n",
      "  Average value of feature pixel_0_7: 0.13\n",
      "  Standard deviation of feature pixel_0_7: 1.04\n",
      "  Average value of feature pixel_1_0: 0.01\n",
      "  Standard deviation of feature pixel_1_0: 0.09\n",
      "  Average value of feature pixel_1_1: 1.99\n",
      "  Standard deviation of feature pixel_1_1: 3.20\n",
      "  Average value of feature pixel_1_2: 10.38\n",
      "  Standard deviation of feature pixel_1_2: 5.42\n",
      "  Average value of feature pixel_1_3: 11.98\n",
      "  Standard deviation of feature pixel_1_3: 3.98\n",
      "  Average value of feature pixel_1_4: 10.28\n",
      "  Standard deviation of feature pixel_1_4: 4.78\n",
      "  Average value of feature pixel_1_5: 8.18\n",
      "  Standard deviation of feature pixel_1_5: 6.05\n",
      "  Average value of feature pixel_1_6: 1.85\n",
      "  Standard deviation of feature pixel_1_6: 3.59\n",
      "  Average value of feature pixel_1_7: 0.11\n",
      "  Standard deviation of feature pixel_1_7: 0.83\n",
      "  Average value of feature pixel_2_0: 0.00\n",
      "  Standard deviation of feature pixel_2_0: 0.06\n",
      "  Average value of feature pixel_2_1: 2.60\n",
      "  Standard deviation of feature pixel_2_1: 3.58\n",
      "  Average value of feature pixel_2_2: 9.90\n",
      "  Standard deviation of feature pixel_2_2: 5.69\n",
      "  Average value of feature pixel_2_3: 6.99\n",
      "  Standard deviation of feature pixel_2_3: 5.80\n",
      "  Average value of feature pixel_2_4: 7.10\n",
      "  Standard deviation of feature pixel_2_4: 6.17\n",
      "  Average value of feature pixel_2_5: 7.81\n",
      "  Standard deviation of feature pixel_2_5: 6.20\n",
      "  Average value of feature pixel_2_6: 1.79\n",
      "  Standard deviation of feature pixel_2_6: 3.26\n",
      "  Average value of feature pixel_2_7: 0.05\n",
      "  Standard deviation of feature pixel_2_7: 0.44\n",
      "  Average value of feature pixel_3_0: 0.00\n",
      "  Standard deviation of feature pixel_3_0: 0.03\n",
      "  Average value of feature pixel_3_1: 2.47\n",
      "  Standard deviation of feature pixel_3_1: 3.15\n",
      "  Average value of feature pixel_3_2: 9.09\n",
      "  Standard deviation of feature pixel_3_2: 6.19\n",
      "  Average value of feature pixel_3_3: 8.82\n",
      "  Standard deviation of feature pixel_3_3: 5.88\n",
      "  Average value of feature pixel_3_4: 9.93\n",
      "  Standard deviation of feature pixel_3_4: 6.15\n",
      "  Average value of feature pixel_3_5: 7.55\n",
      "  Standard deviation of feature pixel_3_5: 5.87\n",
      "  Average value of feature pixel_3_6: 2.32\n",
      "  Standard deviation of feature pixel_3_6: 3.69\n",
      "  Average value of feature pixel_3_7: 0.00\n",
      "  Standard deviation of feature pixel_3_7: 0.05\n",
      "  Average value of feature pixel_4_0: 0.00\n",
      "  Standard deviation of feature pixel_4_0: 0.00\n",
      "  Average value of feature pixel_4_1: 2.34\n",
      "  Standard deviation of feature pixel_4_1: 3.48\n",
      "  Average value of feature pixel_4_2: 7.67\n",
      "  Standard deviation of feature pixel_4_2: 6.32\n",
      "  Average value of feature pixel_4_3: 9.07\n",
      "  Standard deviation of feature pixel_4_3: 6.27\n",
      "  Average value of feature pixel_4_4: 10.30\n",
      "  Standard deviation of feature pixel_4_4: 5.93\n",
      "  Average value of feature pixel_4_5: 8.74\n",
      "  Standard deviation of feature pixel_4_5: 5.87\n",
      "  Average value of feature pixel_4_6: 2.91\n",
      "  Standard deviation of feature pixel_4_6: 3.54\n",
      "  Average value of feature pixel_4_7: 0.00\n",
      "  Standard deviation of feature pixel_4_7: 0.00\n",
      "  Average value of feature pixel_5_0: 0.01\n",
      "  Standard deviation of feature pixel_5_0: 0.15\n",
      "  Average value of feature pixel_5_1: 1.58\n",
      "  Standard deviation of feature pixel_5_1: 2.98\n",
      "  Average value of feature pixel_5_2: 6.88\n",
      "  Standard deviation of feature pixel_5_2: 6.54\n",
      "  Average value of feature pixel_5_3: 7.23\n",
      "  Standard deviation of feature pixel_5_3: 6.44\n",
      "  Average value of feature pixel_5_4: 7.67\n",
      "  Standard deviation of feature pixel_5_4: 6.26\n",
      "  Average value of feature pixel_5_5: 8.24\n",
      "  Standard deviation of feature pixel_5_5: 5.69\n",
      "  Average value of feature pixel_5_6: 3.46\n",
      "  Standard deviation of feature pixel_5_6: 4.33\n",
      "  Average value of feature pixel_5_7: 0.03\n",
      "  Standard deviation of feature pixel_5_7: 0.31\n",
      "  Average value of feature pixel_6_0: 0.01\n",
      "  Standard deviation of feature pixel_6_0: 0.20\n",
      "  Average value of feature pixel_6_1: 0.70\n",
      "  Standard deviation of feature pixel_6_1: 1.75\n",
      "  Average value of feature pixel_6_2: 7.51\n",
      "  Standard deviation of feature pixel_6_2: 5.64\n",
      "  Average value of feature pixel_6_3: 9.54\n",
      "  Standard deviation of feature pixel_6_3: 5.23\n",
      "  Average value of feature pixel_6_4: 9.42\n",
      "  Standard deviation of feature pixel_6_4: 5.30\n",
      "  Average value of feature pixel_6_5: 8.76\n",
      "  Standard deviation of feature pixel_6_5: 6.03\n",
      "  Average value of feature pixel_6_6: 3.73\n",
      "  Standard deviation of feature pixel_6_6: 4.92\n",
      "  Average value of feature pixel_6_7: 0.21\n",
      "  Standard deviation of feature pixel_6_7: 0.98\n",
      "  Average value of feature pixel_7_0: 0.00\n",
      "  Standard deviation of feature pixel_7_0: 0.02\n",
      "  Average value of feature pixel_7_1: 0.28\n",
      "  Standard deviation of feature pixel_7_1: 0.93\n",
      "  Average value of feature pixel_7_2: 5.56\n",
      "  Standard deviation of feature pixel_7_2: 5.10\n",
      "  Average value of feature pixel_7_3: 12.09\n",
      "  Standard deviation of feature pixel_7_3: 4.37\n",
      "  Average value of feature pixel_7_4: 11.81\n",
      "  Standard deviation of feature pixel_7_4: 4.93\n",
      "  Average value of feature pixel_7_5: 6.76\n",
      "  Standard deviation of feature pixel_7_5: 5.90\n",
      "  Average value of feature pixel_7_6: 2.07\n",
      "  Standard deviation of feature pixel_7_6: 4.09\n",
      "  Average value of feature pixel_7_7: 0.36\n",
      "  Standard deviation of feature pixel_7_7: 1.86\n",
      "\n",
      "Dataset Diabetes.\n",
      "Number of samples: 442\n",
      "Target is continuous (no classes).\n",
      "Number of features: 10\n",
      "  Average value of feature age: -0.00\n",
      "  Standard deviation of feature age: 0.05\n",
      "  Average value of feature sex: 0.00\n",
      "  Standard deviation of feature sex: 0.05\n",
      "  Average value of feature bmi: -0.00\n",
      "  Standard deviation of feature bmi: 0.05\n",
      "  Average value of feature bp: -0.00\n",
      "  Standard deviation of feature bp: 0.05\n",
      "  Average value of feature s1: -0.00\n",
      "  Standard deviation of feature s1: 0.05\n",
      "  Average value of feature s2: 0.00\n",
      "  Standard deviation of feature s2: 0.05\n",
      "  Average value of feature s3: -0.00\n",
      "  Standard deviation of feature s3: 0.05\n",
      "  Average value of feature s4: -0.00\n",
      "  Standard deviation of feature s4: 0.05\n",
      "  Average value of feature s5: 0.00\n",
      "  Standard deviation of feature s5: 0.05\n",
      "  Average value of feature s6: 0.00\n",
      "  Standard deviation of feature s6: 0.05\n",
      "\n",
      "Dataset Linnerud.\n",
      "Number of samples: 20\n",
      "Target is multi-output regression with 3 targets.\n",
      "Number of features: 3\n",
      "  Average value of feature Chins: 9.45\n",
      "  Standard deviation of feature Chins: 5.15\n",
      "  Average value of feature Situps: 145.55\n",
      "  Standard deviation of feature Situps: 60.98\n",
      "  Average value of feature Jumps: 70.30\n",
      "  Standard deviation of feature Jumps: 49.98\n",
      "\n",
      "Dataset Wine.\n",
      "Number of samples: 178\n",
      "Number of classes: 3\n",
      "  Number of samples in class class_0: 59\n",
      "  Number of samples in class class_1: 71\n",
      "  Number of samples in class class_2: 48\n",
      "Number of features: 13\n",
      "  Average value of feature alcohol: 13.00\n",
      "  Standard deviation of feature alcohol: 0.81\n",
      "  Average value of feature malic_acid: 2.34\n",
      "  Standard deviation of feature malic_acid: 1.11\n",
      "  Average value of feature ash: 2.37\n",
      "  Standard deviation of feature ash: 0.27\n",
      "  Average value of feature alcalinity_of_ash: 19.49\n",
      "  Standard deviation of feature alcalinity_of_ash: 3.33\n",
      "  Average value of feature magnesium: 99.74\n",
      "  Standard deviation of feature magnesium: 14.24\n",
      "  Average value of feature total_phenols: 2.30\n",
      "  Standard deviation of feature total_phenols: 0.62\n",
      "  Average value of feature flavanoids: 2.03\n",
      "  Standard deviation of feature flavanoids: 1.00\n",
      "  Average value of feature nonflavanoid_phenols: 0.36\n",
      "  Standard deviation of feature nonflavanoid_phenols: 0.12\n",
      "  Average value of feature proanthocyanins: 1.59\n",
      "  Standard deviation of feature proanthocyanins: 0.57\n",
      "  Average value of feature color_intensity: 5.06\n",
      "  Standard deviation of feature color_intensity: 2.31\n",
      "  Average value of feature hue: 0.96\n",
      "  Standard deviation of feature hue: 0.23\n",
      "  Average value of feature od280/od315_of_diluted_wines: 2.61\n",
      "  Standard deviation of feature od280/od315_of_diluted_wines: 0.71\n",
      "  Average value of feature proline: 746.89\n",
      "  Standard deviation of feature proline: 314.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import Bunch\n",
    "import numpy as np\n",
    "\n",
    "def prepare_dataset_description(data_set: Bunch, data_set_name: str):\n",
    "    # Extract required data\n",
    "    data = data_set.data\n",
    "    target = data_set.target\n",
    "    feature_names = data_set.feature_names\n",
    "    num_samples = len(data)\n",
    "    num_features = data.shape[1]\n",
    "    \n",
    "    # Initialize the description\n",
    "    description = f\"Dataset {data_set_name}.\\n\"\n",
    "    description += f\"Number of samples: {num_samples}\\n\"\n",
    "    \n",
    "    # Handle target names and classes (for classification datasets)\n",
    "    if 'target_names' in data_set:\n",
    "        target_names = data_set.target_names\n",
    "        num_classes = len(target_names)\n",
    "        \n",
    "        # For classification, count the number of samples in each class\n",
    "        if target.ndim == 1:  # Only apply bincount if the target is 1D (classification)\n",
    "            class_counts = np.bincount(target)\n",
    "            description += f\"Number of classes: {num_classes}\\n\"\n",
    "            for i, class_name in enumerate(target_names):\n",
    "                description += f\"  Number of samples in class {class_name}: {class_counts[i]}\\n\"\n",
    "        else:\n",
    "            description += f\"Target is multi-output regression with {target.shape[1]} targets.\\n\"\n",
    "    else:\n",
    "        # Handle continuous or multi-output regression targets\n",
    "        if target.ndim == 2:\n",
    "            description += f\"Target is multi-output regression with {target.shape[1]} targets.\\n\"\n",
    "        else:\n",
    "            description += f\"Target is continuous (no classes).\\n\"\n",
    "    \n",
    "    # Feature statistics\n",
    "    description += f\"Number of features: {num_features}\\n\"\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        avg = np.mean(data[:, i])\n",
    "        std = np.std(data[:, i])\n",
    "        description += f\"  Average value of feature {feature_name}: {avg:.2f}\\n\"\n",
    "        description += f\"  Standard deviation of feature {feature_name}: {std:.2f}\\n\"\n",
    "    \n",
    "    return description\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits, load_diabetes, load_linnerud, load_wine\n",
    "\n",
    "# Load the datasets\n",
    "iris_data_set = load_iris()\n",
    "breast_cancer_data_set = load_breast_cancer()\n",
    "digits_data_set = load_digits()\n",
    "diabetes_data_set = load_diabetes()\n",
    "linnerud_data_set = load_linnerud()\n",
    "wine_data_set = load_wine()\n",
    "\n",
    "# Example calls\n",
    "print(prepare_dataset_description(iris_data_set, 'Iris'))\n",
    "print(prepare_dataset_description(breast_cancer_data_set, 'BC'))\n",
    "print(prepare_dataset_description(digits_data_set, 'Digits'))\n",
    "print(prepare_dataset_description(diabetes_data_set, 'Diabetes'))\n",
    "print(prepare_dataset_description(linnerud_data_set, 'Linnerud'))\n",
    "print(prepare_dataset_description(wine_data_set, 'Wine'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cc2e28e15ec3d399ff2fa987eff54814158b78b1ea93d5ce0744d3e4b658fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
