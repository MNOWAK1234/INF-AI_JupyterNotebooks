{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mooJ5vGuciH3"
      },
      "source": [
        "# Atencja — implementacja atencji między danym stanem dekodera a stanami kodera\n",
        "\n",
        "Poniżej znajdują się dwie macierze `encoder_states` i `decoder_states` reprezentujące stan warstwy ukrytej po przetworzeniu każdego słowa przez koder oraz statyczne wektory zanurzeń związane z danym wejściem dekodera. Pojedynczy stan warstwy ukrytej zawiera zanurzenia o długości = 3, która to liczba jest równa rozmiarowi zanurzenia w dekoderze. W koderze mamy 4 stany warstw ukrytych, ponieważ przetwarzamy sekwencję składającą się z 4 tokenów.\n",
        "\n",
        "W dekoderze znajduje się 5 tokenów, które są generowane na podstawie sekwencji przetwarzanej przez koder.\n",
        "\n",
        "# Zadanie 1 (1 punkt)\n",
        "\n",
        "Zadanie polega na: a) Obliczeniu podobieństwa wszystkich zanurzeń z dekodera (zapytań -- queries) do wszystkich zanurzeń kolejnych stanów kodera (kluczy -- keys) (pamiętaj, że macierze można transponować. W NumPy transponujemy macierz za pomocą `nazwa_macierzy. T`)\n",
        "\n",
        "a) Softmax (zaimportowany z scipy) należy wykonać na utworzonej macierzy podobieństw. Uwaga: pamiętaj, aby zastosować softmax we właściwym wymiarze. Wszystkie ukryte stany kodera powinny być zmiękczone w kontekście danego stanu dekodera. W scipy funkcja softmax zawiera argument `axis`, który może pomóc.\n",
        "\n",
        "b) Połącz macierz uwagi z kroku b) i „encoder_states”, aby wygenerować macierz zawierającą wektory kontekstu dla każdego tokena z dekodera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Vbq0QW2td41r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  4.806   2.376   7.762   1.129]\n",
            " [ 12.164 -12.645  73.935   3.636]\n",
            " [ 27.79  -16.962  94.002   7.137]\n",
            " [ 18.702  -5.184  42.616   4.501]\n",
            " [ 64.38   49.86   56.21   14.45 ]]\n",
            "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
            " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
            " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
            " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
            " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
            "[[ 9.69108631  0.35799187  0.59163688]\n",
            " [10.2         0.2         0.3       ]\n",
            " [10.2         0.2         0.3       ]\n",
            " [10.2         0.2         0.3       ]\n",
            " [ 1.20254471  3.39909302  5.59850122]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "# scipy.special.softmax(x, axis=None)\n",
        "\n",
        "encoder_states = np.array(\n",
        "    [[1.2, 3.4, 5.6],   # encoder's hidden layer output at the step 1, related to a given input token, e.g., I\n",
        "    [-2.3, 0.2, 7.2],   # encoder's hidden layer output at the step 2, related to a given token, e.g., like\n",
        "    [10.2, 0.2, 0.3],   # encoder's hidden layer output at the step 3, related to a given token, e.g., NLP\n",
        "    [0.4, 0.7, 1.2]]    # encoder's hidden layer output at the step 4, related to a given token, e.g., \".\"\n",
        ")\n",
        "\n",
        "decoder_states = np.array(\n",
        "    [[0.74, 0.23, 0.56],  # decoder's static word embedding at the step 1, related to a given token, e.g., <BOS>\n",
        "    [7.23, 0.12, 0.55],  # decoder's static word embedding at the step 2, related to a given token, e.g., Ja\n",
        "    [9.12, 4.23, 0.44], # decoder's static word embedding at the step 3, related to a given token, e.g., lubię\n",
        "    [4.1, 3.23, 0.5],    # decoder's static word embedding at the step 4, related to a given token, e.g., przetwarzanie\n",
        "    [5.2, 3.1, 8.5]]     # decoder's static word embedding at the step 5, related to a given token, e.g., języka\n",
        ")\n",
        "\n",
        "# Similarity = dot product\n",
        "attention_scores = np.matmul(decoder_states, encoder_states.T)\n",
        "print(attention_scores)\n",
        "\n",
        "attention_weights = softmax(attention_scores, axis=1)\n",
        "print(attention_weights)\n",
        "\n",
        "context_vectors = np.matmul(attention_weights, encoder_states)\n",
        "print(context_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8bfd5X4fAJq"
      },
      "source": [
        "Oczekiwane wyjścia:\n",
        "\n",
        "a) [[ 4.806 2.376 7.762 1.129] [ 12.164 -12.645 73.935 3.636] [ 27.79 -16.962 94.002 7.137] [ 18.702 -5.184 42.616 4.501] [ 64.38 49.86 56.21 14.45 ]]\n",
        "\n",
        "b) [[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03] [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31] [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38] [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17] [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
        "\n",
        "c) [[ 9.69108631 0.35799187 0.59163688] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [ 1.20254471 3.39909302 5.59850122]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sPUDVw7fKHJ"
      },
      "source": [
        "# Transformer\n",
        "## Wykorzystanie modelu T5 opartego na transformerze do rozwiązywania różnych zadań NLP.\n",
        "\n",
        "Dzisiaj poznamy nową bibliotekę — HuggingFace **transformers** (https://huggingface.co/docs/transformers/index) i użyjemy jej do rozwiązania kilku nieoczywistych problemów związanych z NLP za pomocą modelu **T5** .\n",
        "\n",
        "\n",
        "HuggingFace transformers to jedna z najpopularniejszych bibliotek dostarczających nam wysokopoziomowe API do wykorzystania sieci neuronowych do rozwiązywania zadań związanych z przetwarzaniem języka naturalnego, przetwarzaniem dźwięku, wizją komputerową, a nawet scenariuszami multimodalnymi, w których musimy wykorzystywać wiele modalności naraz (np. odpowiadając na pytania o zdjęcia, wydobywając informacje z faktur).\n",
        "\n",
        "Najpierw zainstalujmy zależności, samą bibliotekę `transformers` oraz moduł `sentencepiece`, który pomaga nam tokenizować dokumenty i przekształcać tokeny w kodowanie one-hot (ideę sentencepiece omówimy szczegółowo później).\n",
        "\n",
        "**Ostrzeżenie**: jeśli zauważysz jakieś dziwne wyjątki, takie jak `cannot call from_pretrained na obiekcie None` gdzieś w twoim kodzie, zrestartuj środowisko używając: Runtime -> restart. Następnie uruchom komórki z kodem (bez ponownej instalacji bibliotek) jeszcze raz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmRzPimhfRja"
      },
      "outputs": [],
      "source": [
        "!pip install transformers   # install HuggingFace transformers library\n",
        "!pip install sentencepiece  # install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSAg0LOEgGj4"
      },
      "source": [
        "Przeczytaj dokumentację dotyczącą modelu T5 dostępne tutaj: https://huggingface.co/docs/transformers/model_doc/t5\n",
        "\n",
        "W sekcji `wnioskowanie` znajdziesz opis pokazujący w jaki sposób możemy pobrać wytrenowany model i użyć go do rozwiązania zadanego zadania. Po prostu użyj dostarczonego kodu, aby przetłumaczyć jakieś zdanie z angielskiego na niemiecki!\n",
        "\n",
        "*(0.5 punkta)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lYOfsrkeJ6GF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lassen Sie mich einfach alleine, ich weiß, was ich tun.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
        "\n",
        "input_ids = tokenizer(\"translate English to German: Just leave me alone, I know what I'm doing.\", return_tensors=\"pt\").input_ids\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z322IklnhQnO"
      },
      "source": [
        "## Różne zadania\n",
        "\n",
        "Eksperymentuj z innymi danymi wejściowymi, np. tymi, które przedstawiono na rysunku 1 przedstawionym w artykule przedstawiającym model T5 lub nawet szerszą listą przypadków użycia z Dodatku D dostarczonego z artykułem. Artykuł można znaleźć tutaj: https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "Uwaga: wśród dostarczonych danych wejściowych zastosowano kilka skrótów, niektóre z nich to:\n",
        "- `stsb`: oznacza semantyczny test porównawczy podobieństwa tekstu. Biorąc pod uwagę dwa zdania, możemy obliczyć ich podobieństwa semantyczne, co może pomóc nam ustalić, czy jedno zdanie jest parafrazą drugiego.\n",
        "- `cola`: oznacza Corpus of Linguistic Acceptability i pomaga nam określić, czy dane zdanie jest gramatyczne, czy niegramatyczne.\n",
        "\n",
        "Jeśli spojrzysz na Dodatek D, skrótów jest więcej, są one związane z nazwami zadań przedstawionymi w benchmarku GLUE (dostępny tutaj: https://gluebenchmark.com/tasks) i benchmarku SUPERGLUE (dostępny tutaj: https:/ /super.gluebenchmark.com/tasks). Ideą GLUE i SUPERGLUE jest zebranie zestawu trudnych zadań, które można wykorzystać do oceny systemów wymagających rozumienia języka naturalnego.\n",
        "\n",
        "**Wklej 3 przykładowe zadania i przetworzone dane wejściowe w komórce poniżej (1 punkt)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OfxmQwV4lIXw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STSB result: 2.4\n",
            "Grammar check result: acceptable\n",
            "Paraphrase result: True\n"
          ]
        }
      ],
      "source": [
        "input_1 = \"stsb sentence1: The cat is sitting on the mat. sentence2: The cat is resting on the rug.\"\n",
        "input_2 = \"cola sentence: He didn't went to the store.\"\n",
        "input_3 = \"paraphrase sentence1: They're having a party! sentence2: A party is happening.\"\n",
        "\n",
        "# Tokenize input data\n",
        "inputs_1 = tokenizer(input_1, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs_2 = tokenizer(input_2, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs_3 = tokenizer(input_3, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "outputs_1 = model.generate(inputs_1.input_ids)\n",
        "outputs_2 = model.generate(inputs_2.input_ids)\n",
        "outputs_3 = model.generate(inputs_3.input_ids)\n",
        "\n",
        "decoded_output_1 = tokenizer.decode(outputs_1[0], skip_special_tokens=True)\n",
        "decoded_output_2 = tokenizer.decode(outputs_2[0], skip_special_tokens=True)\n",
        "decoded_output_3 = tokenizer.decode(outputs_3[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"STSB result:\", decoded_output_1)\n",
        "print(\"Grammar check result:\", decoded_output_2)\n",
        "print(\"Paraphrase result:\", decoded_output_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQvUkC-BlW3q"
      },
      "source": [
        "## Różne typy modeli\n",
        "\n",
        "Dostępnych jest kilka modeli T5, które różnią się wielkością (i jakością). Im większy model, tym lepsze wyjście powinien generować. Eksperymentuj z niektórymi modelami z następującego zestawu:\n",
        "- t5-small\n",
        "- base t5\n",
        "- t5-large\n",
        "- t5-3b\n",
        "- t5-11b\n",
        "\n",
        "Sprawdź, czy można zaobserwować różnicę w jakości generowanych tekstów.\n",
        "\n",
        "Porównaj również rozmiar modeli, możesz użyć funkcji `model.num_parameters()`, aby uzyskać numer parametru związany z każdym modelem. Dla każdego modelu, który jesteś w stanie załadować, podaj rozmiar w poniższej komórce (jeśli nie możesz załadować danego modelu, bo jest za duży, bez obaw, po prostu wpisz 'too big to load'). (*2 punkty*) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1MTMhyyeR3ZJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t5-small params number: 60506624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t5-base params number: 222903552\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub\\models--t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 2950.74 MB. The target location C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub only has 2381.96 MB free disk space.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 2950.74 MB. The target location C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub\\models--t5-large\\blobs only has 2381.96 MB free disk space.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t5-large - too big to load\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub\\models--t5-3b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 11406.46 MB. The target location C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub only has 2.97 MB free disk space.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 11406.46 MB. The target location C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub\\models--t5-3b\\blobs only has 2.97 MB free disk space.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t5-3b - too big to load\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub\\models--t5-11b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 45229.45 MB. The target location C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub only has 0.72 MB free disk space.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Miki Nowak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 45229.45 MB. The target location C:\\Users\\Miki Nowak\\.cache\\huggingface\\hub\\models--t5-11b\\blobs only has 0.72 MB free disk space.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t5-11b - too big to load\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Lista modeli T5 do eksperymentowania\n",
        "models = [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]\n",
        "\n",
        "for model_name in models:\n",
        "    try:\n",
        "        # Ładowanie modelu i tokenizatora\n",
        "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        \n",
        "        # Liczenie parametrów modelu\n",
        "        num_params = model.num_parameters()\n",
        "        \n",
        "        print(f\"{model_name} params number: {num_params}\")\n",
        "    except:\n",
        "        print(f\"{model_name} - too big to load\")\n",
        "\n",
        "\n",
        "# t5-small params number: 60506624\n",
        "# t5-base params number: 222903552\n",
        "# t5-large params number: too big to load\n",
        "# t5-3b params number: too big to load\n",
        "# t5-11b params number: too big to load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDud66l6msUI"
      },
      "source": [
        "## T5 specyficzne dla języka (ZADANIE OPCJONALNE – nie musisz tutaj podawać kodu)\n",
        "\n",
        "Istnieją nawet alternatywy dla oryginalnych modeli T5. Ponieważ model T5 był trenowany na języku angielskim, dostępne są modele specyficzne dla innych języków, np. polskiego (np. plT5 zaproponowany przez Allegro - https://huggingface.co/allegro/plt5-small). Polski model został przeszkolony do rozwiązywania zestawu zadań zebranych w benchmarku KLEJ, który stanowi polską analogię do benchmarku GLUE: https://klejbenchmark.com.\n",
        "\n",
        "Więcej szczegółów na temat plT5 można znaleźć w artykule badawczym: https://arxiv.org/pdf/2205.08808.pdf. Tabela 2 przedstawia przykładowe podpowiedzi, które można wykorzystać do rozwiązania niektórych zadań wymienionych w KLEJ.\n",
        "\n",
        "Możesz wyszukać alternatywę dla oryginalnego T5, na przykład tę związaną z Twoim językiem, i poeksperymentować z nią (**to zadanie nie jest obowiązkowe**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KJZZUkduoEhZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face Transformers models directory not found.\n"
          ]
        }
      ],
      "source": [
        "# (OPTIONAL): If you want, experiment with some alternative models (like language-related, e.g., plT5 related to Polish)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1hsTnxboIe8"
      },
      "source": [
        "## Flan-T5\n",
        "\n",
        "Pod koniec 2022 roku zaproponowano ewolucję T5 o nazwie Flan-T5. Ten model jest również dostarczany przez bibliotekę transformatorów HuggingFace. Odwiedź tę stronę: https://huggingface.co/docs/transformers/model_doc/flan-t5, aby zobaczyć, jak możesz użyć tego modelu (wystarczy zmienić nazwę modelu, aby pobrać!).\n",
        "\n",
        "Flan-T5 jest znacznie potężniejszy niż T5. Możesz zajrzeć do Dodatku D zawartego w artykule opisującym Flan T5, aby zapoznać się z niektórymi formatami wejściowymi (monitami) i generowanymi wartościami. Artykuł jest tutaj: https://arxiv.org/pdf/1910.10683.pdf. Powinieneś skupić się na polach „przetworzonych danych wejściowych”, ponieważ są to reprezentacje używane przez model. Eksperymentuj z wybranymi zadaniami i sprawdź, czy możesz uzyskać takie same wyniki! W poniższym kodzie wklej kod ładujący model Flan-T5 i wykorzystujący go do rozwiązania wybranych zadań. (*1 punkt*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppbmGGqVqERf"
      },
      "outputs": [],
      "source": [
        "# TODO: Here paste some code using Flan-T5 model to solve some task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwyRmBx6q0iT"
      },
      "source": [
        "## (OPCJONALNIE) Dostrajanie\n",
        "\n",
        "Możesz nawet dostroić model T5/Flan-T5, aby rozwiązać wybrane zadanie. Możesz załadować istniejący model T5/Flan-T5, który jest już przeszkolony do rozwiązywania niektórych zadań, i wykorzystać moc 'transfery wiedzy', aby nauczyć go rozwiązywać różne zadania. Jest to o wiele lepsze niż trenowanie sieci od zera i powinno wymagać mniejszej liczby przykładów szkoleniowych.\n",
        "\n",
        "Faza dostrajania jest dość złożona. Jednak opis krok po kroku można znaleźć tutaj: https://www.philschmid.de/fine-tune-flan-t5\n",
        "\n",
        "Możesz spróbować dostroić wybrany model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frt5p4E_rjGd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
