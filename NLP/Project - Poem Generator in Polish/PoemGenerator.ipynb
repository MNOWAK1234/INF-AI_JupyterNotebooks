{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Mikołaj Nowak 151813\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poem Generator in Polish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem I wanted to solve is a Polish poetry generator. It may be difficult to find commercial applications for it, but certain ideas and tools I used to solve this problem could certainly find applications in the broader field of Generative AI.\n",
    "\n",
    "From my experience, language models (as the name suggests) handle language tasks quite well, but they struggle a bit more with modeling analogies and strictly following predefined rules. Their weaknesses are evident in domains such as mathematics or reasoning. If it were possible to compel a model to generate rhymes, thirteen-syllable verses, and middles, the same mechanisms could be applied to represent, for example, arithmetic rules in a knowledge base without relying on contexts but on general knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training data and preprocessing\n",
    "\n",
    "I decided to use the entirety of \"Pan Tadeusz\" as the test data. Other works have slightly different structures and types of rhymes, plus all 13 books provide a substantial amount of data.\n",
    "It would also be useful to have a function for basic text processing, such as removing punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a weird text with punctuation characters\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(word):\n",
    "    return word.translate(str.maketrans('', '', '.,!?:;*«»')).replace('\\n', ' ')\n",
    "\n",
    "example = \"This is. a, wei?!rd te,xt with ...:;punct,,u.a!t!ion characters...\"\n",
    "print(remove_punctuation(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to create thirteen-syllable verses, we should have a function that divides words into syllables. First, we'll define a helper function to correct syllables generated by regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_consonants_at_end(syllable, vowels=\"aąeęiouyó\"):\n",
    "    consonants = 0\n",
    "    for char in reversed(syllable):\n",
    "        if char not in vowels:\n",
    "            consonants += 1\n",
    "        else:\n",
    "            break\n",
    "    return consonants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the actual function for syllable division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Litwo!: ['lit', 'wo']\n",
      "Ojczyzno: ['oj', 'czyz', 'no']\n",
      "moja!: ['mo', 'ja']\n",
      "ty: ['ty']\n",
      "jesteś: ['jes', 'teś']\n",
      "jak: ['jak']\n",
      "zdrowie: ['zdro', 'wie']\n",
      "Ile: ['i', 'le']\n",
      "cię: ['cię']\n",
      "trzeba: ['trze', 'ba']\n",
      "cenić,: ['ce', 'nić']\n",
      "ten: ['ten']\n",
      "tylko: ['tyl', 'ko']\n",
      "się: ['się']\n",
      "dowie: ['do', 'wie']\n",
      "Kto: ['kto']\n",
      "cię: ['cię']\n",
      "stracił.: ['stra', 'cił']\n",
      "Dziś: ['dziś']\n",
      "piękność: ['pięk', 'ność']\n",
      "twą: ['twą']\n",
      "w: []\n",
      "całej: ['ca', 'łej']\n",
      "ozdobie: ['oz', 'do', 'bie']\n",
      "Widzę: ['wi', 'dzę']\n",
      "i: ['i']\n",
      "opisuję,: ['o', 'pi', 'su', 'ję']\n",
      "bo: ['bo']\n",
      "tęsknię: ['tęs', 'knię']\n",
      "po: ['po']\n",
      "tobie.: ['to', 'bie']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def syllabify(word):\n",
    "    word = word.lower()  # Convert to lowercase\n",
    "    word = remove_punctuation(word)\n",
    "    cluster_placeholders = {\n",
    "        \"ch\": \"0\",\n",
    "        \"cz\": \"1\",\n",
    "        \"dz\": \"2\",\n",
    "        \"dź\": \"3\",\n",
    "        \"dż\": \"4\",\n",
    "        \"rz\": \"5\",\n",
    "        \"sz\": \"6\"\n",
    "    }\n",
    "    vowel_clusters = [\"ię\", \"ie\", \"iu\", \"ia\", \"io\", \"ią\", \"ii\"]\n",
    "\n",
    "    # Replace vowel clusters with a placeholder\n",
    "    # Ensure thathe letter \"i\" in \"ię\", \"ie\" etc. is not reated like a singular vowel\n",
    "    for cluster in vowel_clusters:\n",
    "        word = word.replace(cluster, 'I' + cluster[1])\n",
    "    # Replace consonant clusters with unique placeholders to ensure that there are no two-letter consonants\n",
    "    for cluster, placeholder in cluster_placeholders.items():\n",
    "        word = word.replace(cluster, placeholder)\n",
    "\n",
    "    # Regular expression to find syllables\n",
    "    syllables = re.findall(r'[^aąeęiouyó]*[aąeęiouyó][^aąeęiouyóI]*', word)\n",
    "\n",
    "    # Restore vowel clusters\n",
    "    syllables = [syllable.replace('I', 'i') for syllable in syllables]\n",
    "\n",
    "    # Adjust syllables by moving half of the consonants from the end of each syllable to the beginning of the next\n",
    "    for i in range(len(syllables) - 1):\n",
    "        found_consonants = count_consonants_at_end(syllables[i])\n",
    "        consonants_to_move = (found_consonants+1)//2\n",
    "        syllables[i+1] = syllables[i][len(syllables[i])-consonants_to_move:] + syllables[i+1]\n",
    "        syllables[i] = syllables[i][:len(syllables[i])-consonants_to_move]\n",
    "\n",
    "    # Restore consonant clusters\n",
    "    for cluster, placeholder in cluster_placeholders.items():\n",
    "        syllables = [syllable.replace(placeholder, cluster) for syllable in syllables]\n",
    "\n",
    "    return syllables\n",
    "\n",
    "# Example usage\n",
    "text = \"Litwo! Ojczyzno moja! ty jesteś jak zdrowie Ile cię trzeba cenić, ten tylko się dowie Kto cię stracił. Dziś piękność twą w całej ozdobie Widzę i opisuję, bo tęsknię po tobie.\"\n",
    "words = text.split()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word}: {syllabify(word)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can syllabify words now, it would be useful to find their application in generating verses. Therefore, we will also need functions for counting syllables and for checking how much two words rhyme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of syllables in 'trzeba': 2\n",
      "Calculating rhyme factor between 'trzeba' and 'nieba': 1.00\n"
     ]
    }
   ],
   "source": [
    "def count_syllables(word):\n",
    "    return len(syllabify(word))\n",
    "\n",
    "\n",
    "def rhyme_factor(word1, word2):\n",
    "    syllables1 = syllabify(word1)\n",
    "    syllables2 = syllabify(word2)\n",
    "    # Get the last two syllables of each word (if they exist)\n",
    "    lastsyllable1 = syllables1[-1] if len(syllables1) > 0 else 'xxx'\n",
    "    lastsyllable2 = syllables2[-1] if len(syllables2) > 0 else 'xxx'\n",
    "    beforelastsyllable1 = syllables1[-2] if len(syllables1) > 1 else 'xxx'\n",
    "    beforelastsyllable2 = syllables2[-2] if len(syllables2) > 1 else 'xxx'\n",
    "    # Remove consonants only from the beginning of the second last syllable\n",
    "    beforelastsyllable1 = beforelastsyllable1.lstrip('bcdfghjklmnpqrstvwxyz')\n",
    "    beforelastsyllable2 = beforelastsyllable2.lstrip('bcdfghjklmnpqrstvwxyz')\n",
    "    # Combine the last two syllables into endings\n",
    "    ending1 = beforelastsyllable1 + lastsyllable1\n",
    "    ending2 = beforelastsyllable2 + lastsyllable2\n",
    "    # Calculate rhyme factor\n",
    "    min_length = min(len(ending1), len(ending2))\n",
    "    matching_count = sum(1 for c1, c2 in zip(ending1[::-1], ending2[::-1]) if c1 == c2)\n",
    "    return (matching_count / min_length) if min_length > 0 else 0\n",
    "\n",
    "\n",
    "# Example usage\n",
    "word1 = \"trzeba\"\n",
    "word2 = \"nieba\"\n",
    "\n",
    "print(f\"Number of syllables in '{word1}': {count_syllables(word1)}\")\n",
    "print(f\"Calculating rhyme factor between '{word1}' and '{word2}': {rhyme_factor(word1, word2):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word N-grams in generating poems\n",
    "\n",
    "Let's see if we can generate poems using only n-grams. If, after loading the entire \"Pan Tadeusz,\" we manage to create a word database that consistently generates meaningful text, and additionally, in each line, we track the number of syllables and compel the model to rhyme the last one, then we'll have a fully functional poem generator. <br>\n",
    "Let's start by creating a helper function to split text into n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['litwo', 'ojczyzno', 'moja'], ['ojczyzno', 'moja', 'ty'], ['moja', 'ty', 'jesteś'], ['ty', 'jesteś', 'jak'], ['jesteś', 'jak', 'zdrowie']]\n"
     ]
    }
   ],
   "source": [
    "def get_word_ngrams(data, n_gram_len):\n",
    "    data = remove_punctuation(data.lower())\n",
    "    words = data.split(' ')\n",
    "    ngrams = []\n",
    "    for i in range (len(words) - n_gram_len + 1):\n",
    "        ngram = []\n",
    "        for j in range(n_gram_len):\n",
    "            ngram.append(words[i+j])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "print(get_word_ngrams(\"Litwo! Ojczyzno moja! ty jesteś jak zdrowie\", 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's create a function for generating Markov chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def generate_ngram_markov(n_gram_len):\n",
    "    markov_dict = dict()  # Create a dictionary that will map a context (sequence of n-1 words) to a list of allowed next words observed after that context.\n",
    "    with open(\"pan_tadeusz.txt\", 'r', encoding=\"utf8\") as f:  # Read the data corpus.\n",
    "        data = f.read().lower()  # Convert all uppercase letters to lowercase.\n",
    "        data = remove_punctuation(data) #Preprocessing\n",
    "        n_grams = get_word_ngrams(data, n_gram_len)  # Generate all word n-grams from the corpus.\n",
    "        for n_gram in n_grams:  # For each n-gram...\n",
    "            context = \" \".join(n_gram[:-1])  # Take all words from the n-gram except the last one and join them into a single string separated by spaces.\n",
    "            last_word = str(n_gram[-1])  # Take the last word of the n-gram.\n",
    "\n",
    "            if context not in markov_dict.keys():  # If the context without the last word does not exist in the dictionary yet.\n",
    "                markov_dict[context] = list()  # Add it to the dictionary and create a list for it.\n",
    "            markov_dict[context].append(last_word)  # Knowing that the context is in the dictionary, append the last word to the list.\n",
    "\n",
    "    for context in markov_dict.keys():  # For each context (n-1 word sequence).\n",
    "        markov_dict[context] = Counter(markov_dict[context])  # Create a word histogram for words appearing after this context in the corpus.\n",
    "\n",
    "    return markov_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "szlachtaże pierwszy komendant on odpowie majorze  \n",
      "a byłem ci ojcem (mówiąc podkomorzemu jego \n",
      "zamek niech pan bóg mieczem rozciął pysk na łożu  \n",
      "łaskawym chlebie nie miałem zachowanie u \n",
      "szlachty major — jego pamięć i srogość urzędów  \n",
      "urzędów byłam za tą panią ciotką z \n",
      "tyłu jakby w plebanii świéce biega od sztućca  \n",
      "scyzoryka   tam staje pobladły drżący i \n",
      "ziemianinowi ustępować z obrusa poległ  \n",
      "poległ jeden drugiego zachęca dobrzyńscy ja \n",
      "zawsze miłą konewkę swój rydwan orły białe  \n",
      "nasze lance pyta się z drugiej maciej stary \n",
      "ozwały się na głowni rapiera patrzy  \n",
      "patrzy śmiele walczy zawołał cydzik — \n",
      "jutro o przyczynę tak świeżej niepomna przysięgi  \n",
      "przysięgi o filary ten pan rejent i \n",
      "pusta rzekłbyś że mnie cudem (gdy od rana pisał  \n",
      "  ach ja pewną dziewczynkę widziałem w wilnie na \n",
      "nic a na miejscu nieruchomy schyliwszy głowy  \n",
      "głowę mu z niezwyczajnej ich karków rozpuszczają grzywy \n",
      "wstaje i czarny baran lub córki choć się  \n",
      "się snopy zboża malowane na wszystko strwonił \n",
      "choć stary gifrejter co pomyślą więc nic  \n",
      "żyję bo bijatykę lubił bardzo dobitnie \n",
      "malował (był dawniej a wąsy siwe  \n",
      "siwe pokręcił kapoty białe wczesny sztandar \n",
      "wiosny dzwonek znak zgody głowami wreszcie  \n",
      "wreszcie z łogomowicz pamiętacie o robocie \n",
      "i czterech tuzów lud krzyknął w sieni wziął  \n",
      "zmieści się kończą stroić instrumenty już \n",
      "nieraz na rywalów   tak mile i w aktach jeneralności  \n",
      "jeneralności a czynownik i waszeć wczoraj \n",
      "gryzłeś wargi   myślił zrobić wycieczkę porwał  \n",
      "porwał się łąka rząd kosiarzy otawę siekących \n",
      "wciąż kanonada ruskie przysłowie lepsza  \n",
      "lepsza niż godzić pieniaczy ale o tadeusza \n",
      "od słuchaczów przedział więc do pacierza  \n",
      "ojca wygnanego za chłopa równym czy \n",
      "to się z rejentem razem wrzasło wszystko na sam  \n",
      "bok tak paszczęki na łowach w królestwie skrzydlatym \n",
      "złym gatunku czy na kształt sieci w senacie  \n",
      "roku zabłysnąwszy zieloną na koniec nie \n",
      "jest heroiną romansów heroiną romansów  \n",
      "romansów heroiną romansów jest ów \n",
      "mąż wydana wsi kilka szkiców w mateczniku  \n",
      "dno odkryła nieco wylotów kontusza więc \n",
      "ilekroć mówiłam jak róża śród okienicy  \n",
      "swych dowódców przerwany zdradą darmo czekały \n",
      "znawców nikt by donośniej mówić pośpieszał  \n",
      "ale pejzażysty bo przeze drzwi dwoje sam \n",
      "gawęda i policzek ozwały się ciągną  \n",
      "bystrzejsze teraz już teraz ułani pod \n",
      "rękę tak pracują kropiciel i proszę ze  \n",
      "uniżenie ranga moskiewska order cóż \n",
      "gdy polska chociaż ku nim i zaraz ucichnął  \n",
      "że drogą powitam dziedzica któż wojsko całe \n",
      "soplicowo — lecz dał mi się z grotem i ruszył  \n",
      "siostrzenicę gniewa się zalał potem był \n",
      "to daremno a w dobrzynie bracia szlachta była  \n",
      "braty oj wy panowie jak — krzyknęli dąbrowskiego \n",
      "wszystko wkoło mnie kochać i drzwi od  \n",
      "zostawił że im drogę którą w soplicowie obóz \n",
      "gdzie stała blada pędzi na to cięcie znane  \n",
      "znane tylko mam wynagrodzić hojnie dano \n",
      "wódkę jak w młotki ludzi gdyby w lasach siedzi  \n",
      "rozpierzchniony skupia się bratem polluksem jaśnieli \n",
      "na odsiecz niespodziana — rzekła mu cicho  \n",
      "wsadzisz kulę na konia sługi że raz nisko \n",
      " właśnie kiedy śród kwiatów jak wrogi ich domu  \n",
      "za proch papier i brudne jak tadeusz znowu \n",
      "ją rodzice pieścili słaba lękliwa  \n",
      "lękliwa był nalany ode mnie obraził po \n",
      "uroku po niemiecku sędzia niecierpliwi  \n",
      "— rzekł nasuwał kapelusz odwracał się i \n",
      "wyryta chorągiew wielka jak modni wieku  \n",
      "wieku zdrów dlaczego ją szarpać wtem sędziego nauka \n",
      "sama lekko skinął na oko i stąd  \n",
      "w \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "n_gram_len = 2  # Number of words to form an n-gram.\n",
    "markov_dict = generate_ngram_markov(n_gram_len)  # Create a dictionary with word histograms for each context.\n",
    "\n",
    "text = \"szlachta\"  # Text to start generating from.\n",
    "generated = text\n",
    "count = 0\n",
    "line = 1\n",
    "count += sum(count_syllables(word) for word in text.split())\n",
    "to_rhyme = \"\"\n",
    "print(count)\n",
    "\n",
    "for i in range(500):  # Repeat 500 times...\n",
    "    text_spl = text.split(\" \")  # Split the existing text by space (perform a naive tokenization).\n",
    "    context = \" \".join(text_spl[-n_gram_len+1:])  # Get the last n_gram_len - 1 words.\n",
    "    if(line == 1):\n",
    "        idx = random.randrange(sum(markov_dict[context].values()))  # Check which words are allowed as successors to our context and choose one of them randomly according to the distribution created by the histogram.\n",
    "        new_word = next(itertools.islice(markov_dict[context].elements(), idx, None))  # Choose the randomly selected word.\n",
    "        generated = generated + new_word + \" \"\n",
    "        count+=count_syllables(new_word)\n",
    "        if(count > 13):\n",
    "            generated+=\" \\n\"\n",
    "            line = 2\n",
    "            to_rhyme = new_word\n",
    "            count = 0\n",
    "    if(line == 2):\n",
    "        best_word = None\n",
    "        best_rhyme = 0\n",
    "        if(count > 10):\n",
    "            for word, count in markov_dict[context].items():\n",
    "                rhyme = rhyme_factor(word, to_rhyme)\n",
    "                if(rhyme > best_rhyme and word!=to_rhyme):\n",
    "                    best_word = word\n",
    "                    best_rhyme = rhyme\n",
    "            if(best_rhyme > 0.0):\n",
    "                new_word = best_word\n",
    "            else:\n",
    "                idx = random.randrange(sum(markov_dict[context].values()))\n",
    "                new_word = next(itertools.islice(markov_dict[context].elements(), idx, None))\n",
    "            line = 1\n",
    "            count = 0\n",
    "            generated = generated + new_word + \" \\n\"\n",
    "            to_rhyme = \"\"\n",
    "        else:\n",
    "            idx = random.randrange(sum(markov_dict[context].values()))\n",
    "            new_word = next(itertools.islice(markov_dict[context].elements(), idx, None))  \n",
    "            generated = generated + new_word + \" \"\n",
    "        count+=count_syllables(new_word)\n",
    "    text = text + \" \" + new_word  # Append the chosen word at the end.\n",
    "    \n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when loading over 17GB of Polish text, the n-gram approach generates poor text. We could increase the length of the n-grams and use \"Pan Tadeusz\" as the training data, but then we're merely talking about copying the input text, not Generative AI. The n-gram approach is definitely not suitable for generating poems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Pre-built Models\n",
    "\n",
    "Considering that the n-gram approach completely failed, we should definitely employ a pre-built model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a087b062694752adc721e84ad7b85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"ai-forever/mGPT-13B\", num_beams=1, temperature=1.0)\n",
    "generated_text = generator(\"Mały chłopiec\", truncation=True, max_length=50, num_return_sequences=1)\n",
    "for i, text in enumerate(generated_text):\n",
    "  print(i + 1, \":\", text['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Define model and tokenizer names\n",
    "model_name = \"ai-forever/mGPT-13B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load and preprocess text data (replace with your actual file path)\n",
    "with open(\"pan_tadeusz.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preprocess the text (e.g., cleaning, splitting into chunks)\n",
    "def preprocess_text(text):\n",
    "    # Replace this with your specific cleaning/splitting steps\n",
    "    text = text.lower()  # Convert to lowercase (example pre-processing)\n",
    "    sentences = text.split(\". \")  # Split into sentences (example pre-processing)\n",
    "    return sentences\n",
    "\n",
    "sentences = preprocess_text(text)\n",
    "\n",
    "# Prepare training data (tokenization and padding)\n",
    "data = tokenizer(sentences, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Define training arguments (adjust parameters as needed)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,  # Adjust batch size based on your GPU memory\n",
    "    save_steps=10_000,\n",
    "    num_train_epochs=3,  # Adjust training epochs as needed\n",
    ")\n",
    "\n",
    "# Create model and trainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data,\n",
    ")\n",
    "\n",
    "# Train the model (might take a while depending on GPU)\n",
    "trainer.train()\n",
    "\n",
    "print(\"Model fine-tuned successfully!\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"./fine-tuned_model\")\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./fine-tuned_model\")\n",
    "\n",
    "# Text generation function\n",
    "def generate_text(model, tokenizer, prompt, max_length=50, num_return_sequences=1):\n",
    "    \"\"\"Generates text using the fine-tuned model.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    generated_outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        )\n",
    "    decoded_texts = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "    return decoded_texts\n",
    "\n",
    "# Generate text similar to Pan Tadeusz\n",
    "prompt = \"Mały chłopiec\"\n",
    "generated_texts = generate_text(fine_tuned_model, fine_tuned_tokenizer, prompt)\n",
    "\n",
    "# Print generated text\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(i + 1, \":\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested many models capable of generating text in Polish. Downloading the models was a significant time burden, and unfortunately, that's where I lost the most time. The most crucial feature this model needed to possess was high creativity and the ability to generate diverse texts. Some models, even when the temperature was set to the maximum, kept generating the same text. A common problem with all the models I tested was the awareness of context. The longer the poem, the more the model would forget what it was writing about. Therefore, I decided not to generate poems longer than 8 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparision with other available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-3.5-turbo\n",
      "W polu szumiącym brzozowym,\n",
      "Pod niebem błękitnym wiosennym,\n",
      "Głos ptaków słychać wszędzie,\n",
      "Wiatr przepływa jak wiewiórka.\n",
      "Na łące słońce świeci,\n",
      "A ja tam idę wolno,\n",
      "W mym sercu radość rośnie,\n",
      "Bo tu jestem, gdzie kocham.\n",
      "\n",
      "Model: gpt-4\n",
      "Gdzieś między morzem a stromymi wzgórzami,\n",
      "Gdzie kwitną jabłonie złote jak korony,\n",
      "Wróciłam do domu, wśród dawnych drzew starych,\n",
      "Serca pełne tęsknoty, oczu pełne łez i skarg.\n",
      "\n",
      "Niebo jak aksamit, pachnący jasmin krajobraz,\n",
      "Wszystko jak dawniej, lecz serca już nie ma.\n",
      "Kyoły śpiewa nucę stary, polski zdrój,\n",
      "Cichy, jak łza co spływa po policzku mój.\n",
      "\n",
      "Model: gpt-4o\n",
      "O zmierzchu nad wodą, gdzie wierzby szepcą cicho,\n",
      "Młodzieńca serce płacze, lecz dusza pełna licho.\n",
      "Wśród cieni ukochanej, widmo błąka się śpiesznie,\n",
      "Miłość już utracona, choć serce bije grzesznie.\n",
      "\n",
      "Na skrzydłach pieśń niesiona, do gwiazd się wzlatuje,\n",
      "Choć los nam sprzyjał kiedyś, dziś już nie raduje.\n",
      "W blasku księżyca płomień, co w nocy migoce,\n",
      "Tak naszą miłość wspomni, jak rosa na zrosie.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load the API key from the apiKeys.json file\n",
    "with open('apiKeys.json') as f:\n",
    "    api_keys = json.load(f)\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai_api_key = api_keys['openai']['api_key']\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Define the question\n",
    "question = \"Wygeneruj krótki rymowany wiersz w stylu Adama Mickiewicza. Powinien być trzynastozgłoskowcem i składać się z 8 wersów\"\n",
    "\n",
    "# Define a list of language models\n",
    "language_models = [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4o\",\n",
    "]\n",
    "\n",
    "# Loop through each language model\n",
    "for model in language_models:\n",
    "    # Generate text using the specified model and question\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        stream=True,\n",
    "    )\n",
    "    print(f\"Model: {model}\")\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT-4o model is by far the best for the vast majority of tasks related to NLP. <br>\n",
    "And this time it significantly outperforms GPT-3.5 and GPT-4. The verses rhyme in pairs, and the middle rhyme is visible. <br>\n",
    "However, sometimes there are trivial rhymes, and the number of syllables in each verse only fluctuates around 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "For generating poetry, I would definitely use the GPT-4o model provided by the OpenAI API. However, as of now (June 13, 2024), it is likely the best language model in the world for most tasks. However, it is not entirely publicly available, and we have to pay for token generation. If we absolutely need full control, we can try using publicly available models, but we must expect lower quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
