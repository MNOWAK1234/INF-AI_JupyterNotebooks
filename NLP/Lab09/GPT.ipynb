{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0w5N-ULYslb"
      },
      "source": [
        "# Generowanie tekstu za pomocą GPT-2\n",
        "\n",
        "Dzisiaj spróbujemy wygenerować teksty z wykorzystaniem modelu GPT-2 zaproponowanego przez OpenAI. Model ten jest pochodną transformera. Podobnie jak BERT jest jego fraagmentem -- o ile BERT jest koderem z transformera, o tyle GPT -- dekoderem. GPT-2 to wstępnie wytrenowany model, który można pobrać i używać w taki sam sposób jak BERT.\n",
        "\n",
        "Tutaj możesz znaleźć świetne wprowadzenie do ogólnej idei GPT-2: https://jalammar.github.io/illustrated-gpt2/\n",
        "\n",
        "Generalnie jest to model językowy, model, który daje nam prawdopodobieństwo tego, że dany token jest kontynuacją zadanego kontekstu. Na przykład mając następujący kontekst: „Ala ma pięknego”, GPT-2 może oszacować, że istnieje 5%” szans, że następnym słowem będzie kota, i „0,0001%”, że następnym słowem będzie „ma”. `.\n",
        "\n",
        "Wykorzystamy bibliotekę `Huggingface Transformers` do eksperymentowania z GPT-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LwgDoMiINYf"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrgKFTxbasR0"
      },
      "source": [
        "# PODSTAWOWE GENEROWANIE TEKSTU (2 punkty)\n",
        "\n",
        "Zacznijmy od podstawowego scenariusza — ponieważ GPT-2 może obliczyć prawdopodobieństwo wystąpienia następnego słowa po zadanym kontekście, może być używany do generowania tekstów. W bibliotece `transformers` możemy to zrobić dość łatwo. `transformers` zapewnia dostarcza tak zwane potoki, które ukrywają wszystkie warstwy abstrakcji, dzięki czemu możemy generować teksty za pomocą dwóch linii kodu.\n",
        "\n",
        "Przeczytaj dokumentację, która znajduje się tutaj: https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/pipelines, aby zapoznać się z potokami.\n",
        "\n",
        "Następnie wypełnij poniższy kod odpowiednimi fragmentami. W linii 2 skonstruujmy potok typu `text-generation` i ustawmy parametr `model` na `gpt2`.\n",
        "\n",
        "Następnie `generator` można wywoływać w taki sam sposób jak funkcję dając po nim nawiasy okrągłe z parametrami `generator(__tutaj  parametry__)`. Po prostu podaj kilka pierwszych słów tekstu w formie napisu (string) jako pierwszy argument pozycyjny (nie dodawaj spacji na końcu). Możesz podać dodatkowe parametry, takie jak `max_length` (aby ograniczyć długość generowanego tekstu) lub `num_return_sequences` (aby zmusić GPT-2 do wygenerowania wielu tekstów)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "D7QxEKB_IOZ1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : I am the best when it comes to writing about how she has changed. I am sorry I am not being completely honest if you were to hear the word \"feminist\" that often conjures up feelings of sexual and reproductive freedom. However I cannot\n",
            "2 : I am the best when it comes to fighting in the arena! Every game, I want to win the game,\" he said. \"I hope you guys want me to.\" The 6-foot-8 Johnson (42, 174 pounds), however,\n",
            "3 : I am the best when it comes to this kind of content, and I know it will definitely go out in the future.\n",
            "\n",
            "[Laughs.]\n",
            "\n",
            "So this was kind of an effort with the idea to start on a separate project, because\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "generated_text = generator(\"I am the best when it comes to\", truncation = True, max_length=50, num_return_sequences=3)\n",
        "\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tE7Utv2j3k8"
      },
      "source": [
        "Istnieją różne modele oparte na GPT, które są dostępne w bibliotece `transformers`. Tutaj: https://huggingface.co/models?search=gpt, znajdziesz ich listę. Różnią się zestawami danych, na których zostały wytrenowane (oryginalny GPT-2 został wytrenowany na Webtext https://paperswithcode.com/dataset/webtext, który składa się z ~ 40 GB tekstów ściągniętych z Internetu) i rozmiarami modeli (np. GPT2-small składa się z 117M parametrów, GPT2-medium z 345M, GPT2-large z 762M).\n",
        "\n",
        "W zależności od naszych potrzeb i dostępnej pamięci GPU, możemy wybrać odpowiednią wielkość.\n",
        "Istnieją również modele destylowane, które są `skompresowane` podobnie jak np. popularny model DistilBERT: https://huggingface.co/distilgpt2 (Więcej o destylacji znajdziesz tutaj: https://neptune.ai/blog/knowledge-distillation).\n",
        "\n",
        "Sprawdź jak modele o różnej wielkości mają się do jakości generowanych tekstów. Użyj `gpt2-small`, `gpt2-medium`, `gpt2-large` zamiast `gpt2` w potoku i przeanalizuj wyniki.\n",
        "\n",
        "Sprawdź, jak działają modele wytrenowane na bardziej „konkretnych” danych (np.\n",
        "`CodeGPT-small-java-adaptedGPT2`, którego można użyć do pisania kodu w Javie)\n",
        "\n",
        "* Raport z wyników nie jest wymagany. Po prostu poeksperymentuj, jeśli interesuje Cię ten temat :)*\n",
        "\n",
        "\n",
        "# GPT-2 jako źródło wiedzy\n",
        "Ponieważ model dostarcza prawdopodobnych kontynuacji tekstów, możemy na ich podstawie znaleźć odpowiedzi na niektóre pytania.\n",
        "Możesz wpisać `Stolicą Polski jest` jako kontekst, aby sprawdzić, czy zostanie zaproponowana `Warszawa`.\n",
        "\n",
        "(uwaga!: nie dodawaj spacji na końcu kontekstu. Często prowadzi to do... dziwnych rezultatów)\n",
        "\n",
        "Pamiętaj jednak, że internet jest stronniczy. Istnieje wiele prac poświęconych badaniu stronniczości (biasu) modeli GPT. Ten artykuł jest łatwą do przyswojenia analizą problemu http://aclanthology.lst.uni-saarland.de/D19-1339.pdf. Ponieważ te modele są trenowane na treściach generowanych przez ludzi, nie powinniśmy traktować ich jak wyroczni. Zamiast tego powinniśmy traktować je jako pewien model stereotypowego człowieka ;).\n",
        "\n",
        "**Zadanie 2 (1 punkt)\n",
        "Wpisz 5 przykładów wejść, które pokażą potencjalny bias modelu (np. kobieta = nauczycielka, mężczyzna=mechanik)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Przykład 1: Kobieta vs. Mężczyzna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "v6FQrhOeIleW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : A real man should be allowed to be a part of that movement and be able to build their own\n",
            "2 : A real man should be allowed to see that the fact that we have the resources and authority to do\n",
            "3 : A real man should be held to a higher ethical standard. We live in an age when society and\n",
            "1 : A real woman should be ready, able, intelligent, well educated (with a great reputation as,\n",
            "2 : A real woman should be given an opportunity to tell her stories when she becomes pregnant with her first child\n",
            "3 : A real woman should be in every room and at all times – not on the phone with men,\n"
          ]
        }
      ],
      "source": [
        "generated_text = generator(\"A real man should be\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])\n",
        "generated_text = generator(\"A real woman should be\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Przykład 2: Pochodzenie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : Asian people are usually the very least liked and least likely to ever become famous. And I'm not\n",
            "2 : Asian people are usually more affluent and have better salaries than non-Hispanic whites.\n",
            "\n",
            "When asked\n",
            "3 : Asian people are usually young, low middle class, and poor.\n",
            "\n",
            "And they don't get\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : American people are usually afraid of the other side because they think the Americans are bad,\" said Nade\n",
            "2 : American people are usually not the only ones seeing the \"unhappy\" image.\n",
            "\n",
            "\"I\n",
            "3 : American people are usually not going to see these types of ads,\" he said.\n",
            "\n",
            "For his\n",
            "1 : European people are usually not allowed to marry.\n",
            "\n",
            "\"It's a basic democratic right,\" says\n",
            "2 : European people are usually not involved. However, when there are other non-Muslims who cannot access the\n",
            "3 : European people are usually the most optimistic about Brexit. It seems only because of the rise of a tiny\n"
          ]
        }
      ],
      "source": [
        "generated_text = generator(\"Asian people are usually\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])\n",
        "generated_text = generator(\"American people are usually\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])\n",
        "generated_text = generator(\"European people are usually\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Przykład 3: Narodowość"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : German people are known for their patriotism and generosity, while it is true for other religions that they seek\n",
            "2 : German people are known for their love of religion and respect of religious liberty,\" Mr Maitland said\n",
            "3 : German people are known for getting out of their car and driving around. It wasn't clear from reports\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : Polish people are known for being tolerant, welcoming and helpful citizens and they do not want to be\n",
            "2 : Polish people are known for being a highly conservative, religious and anti-religious community. However,\n",
            "3 : Polish people are known for their love of the outdoors and they love their outdoors being surrounded by the\n",
            "1 : Russian people are known for spreading conspiracy theories, such as the \"fake news\" conspiracy, that have\n",
            "2 : Russian people are known for their harsh winters, they were responsible for the deaths of more than 5,\n",
            "3 : Russian people are known for coming out as LGBT — including the person they are. They're just part\n"
          ]
        }
      ],
      "source": [
        "generated_text = generator(\"German people are known for\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])\n",
        "generated_text = generator(\"Polish people are known for\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])\n",
        "generated_text = generator(\"Russian people are known for\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Przykład 4: Rasizm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : Many thieves and murderers share the same physical appearance. Their skin colour is different and their limbs are sometimes\n",
            "2 : Many thieves and murderers share the same physical appearance. Their skin colour is usually darker brown with a dark\n",
            "3 : Many thieves and murderers share the same physical appearance. Their skin colour is very similar to your head.\n"
          ]
        }
      ],
      "source": [
        "generated_text = generator(\"Many thieves and murderers share the same physical appearance. Their skin colour is\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pzykład 5: Stereotypy w pracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : Librarians are usually asked if they have any questions about this website, which is provided for personal\n",
            "2 : Librarians are usually not trained in clinical practice in the legal rights-of-the-pat\n",
            "3 : Librarians are usually not aware of their role in protecting children from serious diseases at a critical time\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : Teachers are usually in session, not in school, so the teachers could leave for their week.\n",
            "2 : Teachers are usually paid at least 30 percent of their salary over two years, rather than the 35\n",
            "3 : Teachers are usually taught this material only when there is some evidence of a difference in attitude or school\n",
            "1 : Comedians are usually drawn to \"comics, manga, literature, video games, music,\n",
            "2 : Comedians are usually very different from average American, though most of them are even slightly different in\n",
            "3 : Comedians are usually very good at getting a high number of results, but it seems common place\n"
          ]
        }
      ],
      "source": [
        "generated_text = generator(\"Librarians are usually\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])\n",
        "generated_text = generator(\"Teachers are usually\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])\n",
        "generated_text = generator(\"Comedians are usually\", truncation = True, max_length=20, num_return_sequences=3)\n",
        "for i, text in enumerate(generated_text):\n",
        "    print(i+1, \":\", text['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7alsiYzOyoCa"
      },
      "source": [
        "# Zachłanne wyszukiwanie vs beam search\n",
        "\n",
        "Domyślny workflow generowania tekstu za pomocą GPT-2 wykorzystuje strategię wyszukiwania zachłannego. Biorąc pod uwagę pewien kontekst, model wybiera token następny patrząc na rozkład prawdopodobieństwa dla tego następnego tokenu. Jednak w tym scenariuszu możemy wygenerować „nieoptymalne” sekwencje. Proszę spojrzeć na tę stronę internetową, aby zrozumieć ideę algorytmu beam search https://huggingface.co/blog/how-to-generate. Krótko mówiąc, beam search zachowuje najbardziej prawdopodobną „liczbę wiązek” hipotez w każdym kroku czasowym i ostatecznie wybiera hipotezę, która ma ogólnie najwyższe prawdopodobieństwo. Dzięki temu jest w stanie spojrzeć nie tylko na bezpośredni następnik, ale również na prawdopodobieństwa kolejnego tokenu.\n",
        "\n",
        "Poniższy kod opisuje alternatywne podejście do korzystania z GPT. Zamiast potoku, tu ręcznie generujemy tokenizator i model, a następnie przekazujemy stokenizowany kontekst do modelu. Proszę spojrzeć na wywołanie funkcji `generate`, można w nim znaleźć parametr `num_beams`, który ustawia liczbę wiązek do zachowania! Spróbuj zmienić ten parametr, aby zobaczyć, jak zmienia się jakość tekstu.\n",
        "\n",
        "**Zadanie 3 (2 punkty): Odpowiedz na pytanie (w komentarzu w kodzie) -- jak parametr num_beams wpływa na jakość tekstu (i dlaczego)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "s6XND_Dk4EF7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The GPT model is great, but there's still a lot of work to be done.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "starting_context = \"The GPT model is great\"\n",
        "\n",
        "input_ids = tokenizer(starting_context, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "outputs = gpt_model.generate(\n",
        "    input_ids,\n",
        "    num_beams=100,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=1,\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**<span style=\"color: #ff0000\"> W praktyce większa liczba wiązek (większa wartość num_beams) może prowadzić do wygenerowania lepszych tekstów, ponieważ model może rozważyć większą liczbę możliwości i uniknąć \"utknięcia\" w lokalnym maksimum. Jednak większa liczba wiązek zazwyczaj wiąże się z wydłużonym czasem generowania tekstu i większym zużyciem pamięci.</span>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u-gyiu-5GG9"
      },
      "source": [
        "# Ograniczanie GPT-2\n",
        "\n",
        "Czasami chcielibyśmy ograniczyć kreatywność wyjścia generowanego przez model. Jeśli używasz modelu GPT2 do pisania komentarzy o swoich produktach, chcesz, aby były pozytywne :). Czy nie byłoby ciekawym zmuszenie GPT-2 do generowania tekstów, które muszą zawierać wybrane słowa typu `cudowny`, `najlepszy` czy `niesamowity`? :).\n",
        "\n",
        "Modele GPT-2 pozwalają na takie ograniczanie wygenerowanego wyjścia. Dobre wprowadzenie można znaleźć tutaj: https://towardsdatascience.com/new-hugging-face-feature-constrained-beam-search-with-transformers-7ebcfc2d70e9\n",
        ".\n",
        "\n",
        "Przeanalizuj poniższy fragment kodu (zmodyfikowany kod ze wspomnianej powyżej strony internetowej), aby zobaczyć, jak możemy zmusić GPT-2 do korzystania z niektórych tokenów. Są 2 przypadki:\n",
        "* podajemy jakiś pojedynczy token, który musi znaleźć się gdzieś w generowanym tekście\n",
        "* podajemy listę alternatyw, z których model GPT-2 wybiera jedną.\n",
        "\n",
        "Ważna uwaga: podczas eksperymentowania z kodem zauważyłem kiedyś, że model wygenerował `besting` zamiast oczekiwanego słowa `best`. Na początku byłem zaskoczony, ale działa to dobrze: podczas gdy „best” jest tokenem, którego oczekujemy, że będzie obecny w generowanym tekście, we wstępnie wytrenowanych modelach związanych z transformatorami używamy tokenizacji, która może generować jednostki słów podrzędnych (subword units). Jeśli po `best` zostanie wygenerowany subtoken będący kontynuacją (np. `##ing` zgodnie z notacją WordPiece używaną w BERT), to tokeny te zostaną połączone. To nie powoduje, że wynik jest błędny — token `best` jest zawarty w wygenerowanym tekście!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Q4HFp4rfIstU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The laptop is powered by an Intel Core i7-4790K CPU, which has amazing best\n",
            "The product is available in a variety of colors and sizes, including the standard black. The beautiful best\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "must_contain_token = \"best\"\n",
        "must_contain_alternatives = [\"amazing\", \"wonderful\", \"beautiful\", \"exceptional\"]  # let gpt choose which word to use\n",
        "\n",
        "\n",
        "force_words_ids = [\n",
        "    tokenizer([must_contain_token], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
        "    tokenizer(must_contain_alternatives, add_prefix_space=True, add_special_tokens=False).input_ids,\n",
        "]\n",
        "\n",
        "starting_text = [\"The laptop\", \"The product\"]\n",
        "input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "outputs = gpt_model.generate(\n",
        "    input_ids,\n",
        "    force_words_ids=force_words_ids,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=1,\n",
        "    remove_invalid_values=True,\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(tokenizer.decode(outputs[1], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgPecOwX8svn"
      },
      "source": [
        "W 2020 roku powstała nowa wersja o nazwie GPT3. Chociaż OpenAI nie opublkowało modelu do ściągnięcia, zapewniono dostęp do modelu jedynie za pośrednictwem interfejsu API, podejmowane są pewne próby replikacji modelu. Model, który powinien działać tak samo jak GPT3, znajdziesz tutaj: https://huggingface.co/EleutherAI/gpt-neo-1.3B.\n",
        "Historia GPT3 i powody, dla których nie jest on publikowany jako model do pobrania, są opisane w Wikipedii: https://en.wikipedia.org/wiki/GPT-3.\n",
        "\n",
        "W ostatnich miesiącach pojawiły się również alternatywy dla modelu GPT-4, ktory również nie jest dostępny do ściągnięcia. Ciekawym modelem jest mini-GPT4, który można znaleźć tu: https://minigpt-4.github.io"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
